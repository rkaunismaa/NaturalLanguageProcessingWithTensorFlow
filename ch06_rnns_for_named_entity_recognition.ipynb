{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3fd6da",
   "metadata": {},
   "source": [
    "# Named Entity Recognition with RNNs\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/packt_nlp_tensorflow_2/blob/master/Ch06-Recurrent-Neural-Networks/ch06_rnns_for_named_entity_recognition.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f9355",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1556d429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 54321\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0bd6a8",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ebbc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified data\\conllpp_train.txt\n",
      "Found and verified data\\conllpp_dev.txt\n",
      "Found and verified data\\conllpp_test.txt\n"
     ]
    }
   ],
   "source": [
    "url = 'https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/'\n",
    "dir_name = 'data'\n",
    "#https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/conllpp_train.txt\n",
    "def download_data(url, filename, download_dir, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "      \n",
    "    # Create directories if doesn't exist\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    # If file doesn't exist download\n",
    "    if not os.path.exists(os.path.join(download_dir,filename)):\n",
    "        filepath, _ = urlretrieve(url + filename, os.path.join(download_dir,filename))\n",
    "    else:\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "    \n",
    "    # Check the file size\n",
    "    statinfo = os.stat(filepath)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filepath)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filepath + '. Can you get to it with a browser?')\n",
    "        \n",
    "    return filepath\n",
    "\n",
    "# Filepaths to train/valid/test data\n",
    "train_filepath = download_data(url, 'conllpp_train.txt', dir_name, 3283420)\n",
    "dev_filepath = download_data(url, 'conllpp_dev.txt', dir_name, 827443)\n",
    "test_filepath = download_data(url, 'conllpp_test.txt', dir_name, 748737)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a1cee",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f5f0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data ...\n",
      "\tDone\n",
      "Reading data ...\n",
      "\tDone\n",
      "Reading data ...\n",
      "\tDone\n",
      "Train size: 14041\n",
      "Valid size: 3250\n",
      "Test size: 3452\n",
      "\n",
      "Sample data\n",
      "\n",
      "Sentence: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
      "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence: LONDON 1996-08-30\n",
      "Labels: ['B-LOC', 'O']\n",
      "\n",
      "\n",
      "Sentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\n",
      "Labels: ['B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
      "\n",
      "\n",
      "Sentence: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
      "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    '''\n",
    "    Read data from a file with given filename\n",
    "    Returns a list of sentences (each sentence a string), \n",
    "    and list of ner labels for each string\n",
    "    '''\n",
    "\n",
    "    print(\"Reading data ...\")\n",
    "    # master lists - Holds sentences (list of tokens), ner_labels (for each token an NER label)\n",
    "    sentences, ner_labels = [], [] \n",
    "    \n",
    "    # Open the file\n",
    "    with open(filename,'r',encoding='latin-1') as f:        \n",
    "        # Read each line\n",
    "        is_sos = True # We record at each line if we are seeing the beginning of a sentence\n",
    "        \n",
    "        # Tokens and labels of a single sentence, flushed when encountered a new one\n",
    "        sentence_tokens = []\n",
    "        sentence_labels = []\n",
    "        i = 0\n",
    "        for row in f:\n",
    "            # If we are seeing an empty line or -DOCSTART- that's a new line\n",
    "            if len(row.strip()) == 0 or row.split(' ')[0] == '-DOCSTART-':\n",
    "                is_sos = False\n",
    "            # Otherwise keep capturing tokens and labels\n",
    "            else:\n",
    "                is_sos = True\n",
    "                token, _, _, ner_label = row.split(' ')\n",
    "                sentence_tokens.append(token)\n",
    "                sentence_labels.append(ner_label.strip())\n",
    "            \n",
    "            # When we reach the end / or reach the beginning of next\n",
    "            # add the data to the master lists, flush the temporary one\n",
    "            if not is_sos and len(sentence_tokens)>0:\n",
    "                sentences.append(' '.join(sentence_tokens))\n",
    "                ner_labels.append(sentence_labels)\n",
    "                sentence_tokens, sentence_labels = [], []\n",
    "    \n",
    "    print('\\tDone')\n",
    "    return sentences, ner_labels\n",
    "\n",
    "# Train data\n",
    "train_sentences, train_labels = read_data(train_filepath) \n",
    "# Validation data\n",
    "valid_sentences, valid_labels = read_data(dev_filepath) \n",
    "# Test data\n",
    "test_sentences, test_labels = read_data(test_filepath) \n",
    "\n",
    "# Print some stats\n",
    "print(f\"Train size: {len(train_labels)}\")\n",
    "print(f\"Valid size: {len(valid_labels)}\")\n",
    "print(f\"Test size: {len(test_labels)}\")\n",
    "\n",
    "# Print some data\n",
    "print('\\nSample data\\n')\n",
    "for v_sent, v_labels in zip(valid_sentences[:5], valid_labels[:5]):\n",
    "    print(f\"Sentence: {v_sent}\")\n",
    "    print(f\"Labels: {v_labels}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53895ded",
   "metadata": {},
   "source": [
    "## Checking the balance of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0602956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data label counts\n",
      "O         169578\n",
      "B-LOC       7140\n",
      "B-PER       6600\n",
      "B-ORG       6321\n",
      "I-PER       4528\n",
      "I-ORG       3704\n",
      "B-MISC      3438\n",
      "I-LOC       1157\n",
      "I-MISC      1155\n",
      "dtype: int64\n",
      "\n",
      "Validation data label counts\n",
      "O         42759\n",
      "B-PER      1842\n",
      "B-LOC      1837\n",
      "B-ORG      1341\n",
      "I-PER      1307\n",
      "B-MISC      922\n",
      "I-ORG       751\n",
      "I-MISC      346\n",
      "I-LOC       257\n",
      "dtype: int64\n",
      "\n",
      "Test data label counts\n",
      "O         38143\n",
      "B-ORG      1714\n",
      "B-LOC      1645\n",
      "B-PER      1617\n",
      "I-PER      1161\n",
      "I-ORG       881\n",
      "B-MISC      722\n",
      "I-LOC       259\n",
      "I-MISC      252\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Print the value count for each label\n",
    "print(\"Training data label counts\")\n",
    "print(pd.Series(chain(*train_labels)).value_counts())\n",
    "\n",
    "print(\"\\nValidation data label counts\")\n",
    "print(pd.Series(chain(*valid_labels)).value_counts())\n",
    "\n",
    "print(\"\\nTest data label counts\")\n",
    "print(pd.Series(chain(*test_labels)).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47c16e",
   "metadata": {},
   "source": [
    "## Analysing the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a793c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14041.000000\n",
       "mean        14.501887\n",
       "std         11.602756\n",
       "min          1.000000\n",
       "5%           2.000000\n",
       "50%         10.000000\n",
       "95%         37.000000\n",
       "max        113.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_sentences).str.split().str.len().describe(percentiles=[0.05, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971a00a",
   "metadata": {},
   "source": [
    "## Padding/Truncating sentences to create arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f68a579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n"
     ]
    }
   ],
   "source": [
    "def get_label_id_map(train_labels):\n",
    "    # Get the unique list of labels\n",
    "    unique_train_labels = pd.Series(chain(*train_labels)).unique()\n",
    "    # Create a class label -> class ID mapping\n",
    "    labels_map = dict(zip(unique_train_labels, np.arange(unique_train_labels.shape[0])))\n",
    "    print(f\"labels_map: {labels_map}\")\n",
    "    return labels_map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_padded_int_labels(labels, labels_map, max_seq_length, return_mask=True):\n",
    "\n",
    "    # Convert string labels to integers \n",
    "    int_labels = [[labels_map[x] for x in one_seq] for one_seq in labels]\n",
    "    \n",
    "    \n",
    "    # Pad sequences\n",
    "    if return_mask:\n",
    "        # If we return mask, we first pad with a special value (-1) and \n",
    "        # use that to create the mask and later replace -1 with 'O'\n",
    "        padded_labels = np.array(\n",
    "            tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                int_labels, maxlen=max_seq_length, padding='post', truncating='post', value=-1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # mask filter\n",
    "        mask_filter = (padded_labels != -1)\n",
    "        # replace -1 with 'O' s ID\n",
    "        padded_labels[~mask_filter] = labels_map['O']        \n",
    "        return padded_labels, mask_filter.astype('int')\n",
    "    \n",
    "    else:\n",
    "        padded_labels = np.array(ner_pad_sequence_func(int_labels, value=labels_map['O']))\n",
    "        return padded_labels\n",
    "\n",
    "    \n",
    "labels_map = get_label_id_map(train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c01894",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e7c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of sequences\n",
    "max_seq_length = 40\n",
    "\n",
    "# Size of token embeddings\n",
    "embedding_size = 64\n",
    "\n",
    "# Number of hidden units in the RNN layer\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "# Number of output nodes in the last layer\n",
    "n_classes = 9\n",
    "\n",
    "# Number of samples in a batch\n",
    "batch_size = 64\n",
    "\n",
    "# Number of epochs to train\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252c9f9",
   "metadata": {},
   "source": [
    "## Processing the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633ee8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1]\n",
      " [3 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1]]\n",
      "[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert string labels to integers for all train/validation/test data\n",
    "# Pad train/validation/test data\n",
    "padded_train_labels, train_mask = get_padded_int_labels(\n",
    "    train_labels, labels_map, max_seq_length, return_mask=True\n",
    ")\n",
    "padded_valid_labels, valid_mask = get_padded_int_labels(\n",
    "    valid_labels, labels_map, max_seq_length, return_mask=True\n",
    ")\n",
    "padded_test_labels, test_mask  = get_padded_int_labels(\n",
    "    test_labels, labels_map, max_seq_length, return_mask=True\n",
    ")\n",
    "\n",
    "\n",
    "# Print some labels IDs\n",
    "print(padded_train_labels[:2])\n",
    "print(train_mask[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393199a",
   "metadata": {},
   "source": [
    "## Introduction to the `TextVectorization` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d798dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With default arguments\n",
      "\n",
      "Data: \n",
      "[[ 9  4  6  2  3  8  7]\n",
      " [ 2  3  5 10  0  0  0]]\n",
      "Vocabulary: ['', '[UNK]', 'the', 'market', 'went', 'was', 'to', 'sunday', 'on', 'i', 'empty']\n",
      "--------------------------------------------------\n",
      "\n",
      "With limited vocabulary\n",
      "\n",
      "Data: \n",
      "[[1 4 1 2 3 1 1]\n",
      " [2 3 1 1 0 0 0]]\n",
      "Vocabulary: ['', '[UNK]', 'the', 'market', 'went']\n",
      "--------------------------------------------------\n",
      "\n",
      "With preprocessing disabled\n",
      "\n",
      "Data: \n",
      "[[12  2  4  5  7  6 10]\n",
      " [ 9 11  3  8  0  0  0]]\n",
      "Vocabulary: ['', '[UNK]', 'went', 'was', 'to', 'the', 'on', 'market', 'empty.', 'The', 'Sunday', 'Market', 'I']\n",
      "--------------------------------------------------\n",
      "\n",
      "With a maximum sequence length\n",
      "\n",
      "Data: \n",
      "[[ 9  4  6  2]\n",
      " [ 2  3  5 10]]\n",
      "Vocabulary: ['', '[UNK]', 'the', 'market', 'went', 'was', 'to', 'sunday', 'on', 'i', 'empty']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "toy_corpus = [\"I went to the market on Sunday\", \"The Market was empty.\"]\n",
    "toy_vectorization_layer = TextVectorization()\n",
    "\n",
    "# Fit it on a corpus of data\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "\n",
    "toy_vectorized_output = toy_vectorization_layer(toy_corpus)\n",
    "toy_vocabulary = toy_vectorization_layer.get_vocabulary()\n",
    "\n",
    "print(\"With default arguments\\n\")\n",
    "print(f\"Data: \\n{toy_vectorized_output}\")\n",
    "print(f\"Vocabulary: {toy_vocabulary}\")\n",
    "print('-'*50)\n",
    "\n",
    "toy_vectorization_layer = TextVectorization(max_tokens=5)\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "\n",
    "print(\"\\nWith limited vocabulary\\n\")\n",
    "print(f\"Data: \\n{toy_vectorization_layer(toy_corpus)}\")\n",
    "print(f\"Vocabulary: {toy_vectorization_layer.get_vocabulary()}\")\n",
    "print('-'*50)\n",
    "\n",
    "toy_vectorization_layer = TextVectorization(standardize=None)\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "\n",
    "print(\"\\nWith preprocessing disabled\\n\")\n",
    "print(f\"Data: \\n{toy_vectorization_layer(toy_corpus)}\")\n",
    "print(f\"Vocabulary: {toy_vectorization_layer.get_vocabulary()}\")\n",
    "print('-'*50)\n",
    "\n",
    "toy_vectorization_layer = TextVectorization(output_sequence_length=4)\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "\n",
    "print(\"\\nWith a maximum sequence length\\n\")\n",
    "print(f\"Data: \\n{toy_vectorization_layer(toy_corpus)}\")\n",
    "print(f\"Vocabulary: {toy_vectorization_layer.get_vocabulary()}\")\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45477734",
   "metadata": {},
   "source": [
    "## Implement a recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d7f471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x0000026C5306B9D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "def get_fitted_token_vectorization_layer(corpus, max_seq_length, vocabulary_size=None):\n",
    "    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n",
    "    \n",
    "    # Define a text vectorization layer\n",
    "    vectorization_layer = TextVectorization(\n",
    "        max_tokens=vocabulary_size, standardize=None,        \n",
    "        output_sequence_length=max_seq_length, \n",
    "    )\n",
    "    # Fit it on a corpus of data\n",
    "    vectorization_layer.adapt(corpus)\n",
    "    \n",
    "    # Get the vocabulary size\n",
    "    n_vocab = len(vectorization_layer.get_vocabulary())\n",
    "\n",
    "    return vectorization_layer, n_vocab\n",
    "\n",
    "\n",
    "# Input layer\n",
    "word_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "# Text vectorize layer\n",
    "vectorize_layer, n_vocab = get_fitted_token_vectorization_layer(train_sentences, max_seq_length)\n",
    "\n",
    "# Vectorized output (each word mapped to an int ID)\n",
    "vectorized_out = vectorize_layer(word_input)\n",
    "\n",
    "# Look up embeddings for the returned IDs\n",
    "embedding_layer = layers.Embedding(input_dim=n_vocab, output_dim=embedding_size, mask_zero=True)(vectorized_out)\n",
    "\n",
    "# Define a simple RNN layer, it returns an output at each position\n",
    "rnn_layer = layers.SimpleRNN(\n",
    "    units=rnn_hidden_size, return_sequences=True\n",
    ")\n",
    "\n",
    "rnn_out = rnn_layer(embedding_layer)\n",
    "\n",
    "dense_layer = layers.Dense(n_classes, activation='softmax')\n",
    "dense_out = dense_layer(rnn_out)\n",
    "\n",
    "model = tf.keras.Model(inputs=word_input, outputs=dense_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89071c77",
   "metadata": {},
   "source": [
    "## Defining a custom metric and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c303ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_accuracy(y_true, y_pred):\n",
    "    \n",
    "    # [batch size * time]\n",
    "    y_true = tf.cast(tf.reshape(y_true, [-1]), 'int32')\n",
    "    y_pred = tf.cast(tf.reshape(tf.argmax(y_pred, axis=-1), [-1]), 'int32')\n",
    "    \n",
    "    sorted_y_true = tf.sort(y_true)\n",
    "    sorted_inds = tf.argsort(y_true)\n",
    "    \n",
    "    sorted_y_pred = tf.gather(y_pred, sorted_inds)\n",
    "    \n",
    "    sorted_correct = tf.cast(tf.math.equal(sorted_y_true, sorted_y_pred), 'int32')\n",
    "    \n",
    "    # We are adding one to make sure ther eare no division by zero\n",
    "    correct_for_each_label = tf.cast(tf.math.segment_sum(sorted_correct, sorted_y_true), 'float32') + 1\n",
    "    all_for_each_label = tf.cast(tf.math.segment_sum(tf.ones_like(sorted_y_true), sorted_y_true), 'float32') + 1\n",
    "    \n",
    "    mean_accuracy = tf.reduce_mean(correct_for_each_label/all_for_each_label)\n",
    "    \n",
    "    return mean_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e60dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 40)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 40, 64)            1512000   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 40, 64)            8256      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 40, 9)             585       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,520,841\n",
      "Trainable params: 1,520,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy, name='macro_accuracy')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[mean_accuracy_metric])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a410a",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "When training the model we will use `sample_weight` to counteract class-imbalance. We will not use `class_weight`. When using `class_weight` as follows,\n",
    "\n",
    "```\n",
    "model.fit(\n",
    "        train_sentences[i:i+1], padded_train_labels[i:i+1], \n",
    "        class_weight=train_class_weights,\n",
    "        batch_size=64,\n",
    "        epochs=3, \n",
    "        validation_data=(np.array(valid_sentences), padded_valid_labels)\n",
    ")\n",
    "```\n",
    "\n",
    "it leads to the error below,\n",
    "\n",
    "```\n",
    "InvalidArgumentError: 2 root error(s) found.\n",
    "  (0) Invalid argument:  indices[0] = 11 is not in [0, 9)\n",
    "\t [[{{node GatherV2}}]]\n",
    "\t [[IteratorGetNext]]\n",
    "  (1) Invalid argument:  indices[0] = 11 is not in [0, 9)\n",
    "\t [[{{node GatherV2}}]]\n",
    "\t [[IteratorGetNext]]\n",
    "\t [[model/text_vectorization/cond/then/_0/model/text_vectorization/cond/Pad/_56]]\n",
    "0 successful operations.\n",
    "0 derived errors ignored. [Op:__inference_train_function_7453]\n",
    "\n",
    "Function call stack:\n",
    "train_function -> train_function\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97ab9018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n",
      "Class weights: {1: 0.006811025015037328, 5: 0.16176470588235295, 3: 0.17500000000000002, 0: 0.18272425249169436, 4: 0.25507950530035334, 6: 0.31182505399568033, 2: 0.33595113438045376, 8: 0.9982713915298186, 7: 1.0}\n"
     ]
    }
   ],
   "source": [
    "def get_class_weights(train_labels):\n",
    "    \n",
    "    label_count_ser = pd.Series(chain(*train_labels)).value_counts()\n",
    "    label_count_ser = label_count_ser.sum()/label_count_ser\n",
    "    label_count_ser /= label_count_ser.max()\n",
    "    \n",
    "    label_id_map = get_label_id_map(train_labels)\n",
    "    label_count_ser.index = label_count_ser.index.map(label_id_map)\n",
    "    return label_count_ser.to_dict()\n",
    "\n",
    "def get_sample_weights_from_class_weights(labels, class_weights):\n",
    "    \"\"\" From the class weights generate sample weights \"\"\"\n",
    "    return np.vectorize(class_weights.get)(labels)\n",
    "\n",
    "\n",
    "train_class_weights = get_class_weights(train_labels)\n",
    "print(f\"Class weights: {train_class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4b5988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "220/220 [==============================] - 52s 210ms/step - loss: 0.0294 - macro_accuracy: 0.5621 - val_loss: 0.4052 - val_macro_accuracy: 0.7130\n",
      "Epoch 2/3\n",
      "220/220 [==============================] - 45s 206ms/step - loss: 0.0092 - macro_accuracy: 0.8872 - val_loss: 0.1782 - val_macro_accuracy: 0.8001\n",
      "Epoch 3/3\n",
      "220/220 [==============================] - 45s 206ms/step - loss: 0.0032 - macro_accuracy: 0.9586 - val_loss: 0.1204 - val_macro_accuracy: 0.8109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c53051b08>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Make train_sequences an array\n",
    "train_sentences = np.array(train_sentences)\n",
    "# Get sample weights (we cannot use class_weight with TextVectorization layer)\n",
    "train_sample_weights = get_sample_weights_from_class_weights(padded_train_labels, train_class_weights)\n",
    "\n",
    "# Training the model\n",
    "model.fit(\n",
    "        train_sentences, padded_train_labels, \n",
    "        sample_weight=train_sample_weights,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs, \n",
    "        validation_data=(np.array(valid_sentences), padded_valid_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c57948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 3s 32ms/step - loss: 0.1287 - macro_accuracy: 0.7804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1287258118391037, 0.7803568840026855]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(test_sentences), padded_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12141b51",
   "metadata": {},
   "source": [
    "## Visually analysing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "217ce397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\t SOCCER\t-\tJAPAN\tGET\tLUCKY\tWIN\t,\tCHINA\tIN\tSURPRISE\tDEFEAT\t.\n",
      "True:\t O\tO\tB-LOC\tO\tO\tO\tO\tB-LOC\tO\tO\tO\tO\n",
      "Pred:\t O\tO\tB-ORG\tI-PER\tO\tO\tO\tB-PER\tO\tB-ORG\tI-ORG\tO\n",
      "\n",
      "\n",
      "Sample:\t Nadim\tLadki\n",
      "True:\t B-PER\tI-PER\n",
      "Pred:\t B-ORG\tI-ORG\n",
      "\n",
      "\n",
      "Sample:\t AL-AIN\t,\tUnited\tArab\tEmirates\t1996-12-06\n",
      "True:\t B-LOC\tO\tB-LOC\tI-LOC\tI-LOC\tO\n",
      "Pred:\t B-ORG\tO\tB-LOC\tI-LOC\tI-LOC\tI-ORG\n",
      "\n",
      "\n",
      "Sample:\t Japan\tbegan\tthe\tdefence\tof\ttheir\tAsian\tCup\ttitle\twith\ta\tlucky\t2-1\twin\tagainst\tSyria\tin\ta\tGroup\tC\tchampionship\tmatch\ton\tFriday\t.\n",
      "True:\t B-LOC\tO\tO\tO\tO\tO\tB-MISC\tI-MISC\tO\tO\tO\tO\tO\tO\tO\tB-LOC\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "Pred:\t B-LOC\tO\tO\tO\tO\tO\tB-MISC\tI-MISC\tI-MISC\tO\tO\tO\tO\tO\tO\tB-LOC\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "\n",
      "Sample:\t But\tChina\tsaw\ttheir\tluck\tdesert\tthem\tin\tthe\tsecond\tmatch\tof\tthe\tgroup\t,\tcrashing\tto\ta\tsurprise\t2-0\tdefeat\tto\tnewcomers\tUzbekistan\t.\n",
      "True:\t O\tB-LOC\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tB-LOC\tO\n",
      "Pred:\t O\tB-LOC\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tB-PER\tO\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = 5\n",
    "visual_test_sentences = test_sentences[:n_samples]\n",
    "visual_test_labels = padded_test_labels[:n_samples]\n",
    "\n",
    "visual_test_predictions = model.predict(np.array(visual_test_sentences))\n",
    "visual_test_pred_labels = np.argmax(visual_test_predictions, axis=-1)\n",
    "\n",
    "rev_labels_map = dict(zip(labels_map.values(), labels_map.keys()))\n",
    "for i, (sentence, sent_labels, sent_preds) in enumerate(zip(visual_test_sentences, visual_test_labels, visual_test_pred_labels)):    \n",
    "    n_tokens = len(sentence.split())\n",
    "    print(\"Sample:\\t\",\"\\t\".join(sentence.split()))\n",
    "    print(\"True:\\t\",\"\\t\".join([rev_labels_map[i] for i in sent_labels[:n_tokens]]))\n",
    "    print(\"Pred:\\t\",\"\\t\".join([rev_labels_map[i] for i in sent_preds[:n_tokens]]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030ff12",
   "metadata": {},
   "source": [
    "## Defining an advance RNN model\n",
    "\n",
    "* Token embeddings + Char embeddings\n",
    "* Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b79b1a",
   "metadata": {},
   "source": [
    "### Statistics about token lenghts (for char embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "553b1654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    23623.000000\n",
       "mean         6.832705\n",
       "std          2.749288\n",
       "min          1.000000\n",
       "5%           3.000000\n",
       "50%          7.000000\n",
       "95%         12.000000\n",
       "max         61.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_ser = pd.Series(pd.Series(train_sentences).str.split().explode().unique())\n",
    "vocab_ser.str.len().describe(percentiles=[0.05, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37938ac8",
   "metadata": {},
   "source": [
    "## Testing `TextVectorization` for char level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c07fc70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequence: [[['aaaa'], ['bb'], ['c']], [['d'], ['eee'], ['']]]\n",
      "Vectorized output: [[[2 2 2 2]\n",
      "  [4 4 0 0]\n",
      "  [6 0 0 0]]\n",
      "\n",
      " [[5 0 0 0]\n",
      "  [3 3 3 0]\n",
      "  [0 0 0 0]]]\n",
      "Vocabulary: ['', '[UNK]', 'a', 'e', 'b', 'd', 'c']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def split_char(token):\n",
    "    \"\"\" Instead of splitting word by word, split each char\"\"\"\n",
    "    return tf.strings.bytes_split(token)\n",
    "\n",
    "\n",
    "# Define a vectorization layer that splits chars\n",
    "vectorization_layer = TextVectorization(\n",
    "        standardize=None,      \n",
    "        split=split_char,\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_corpus_for_char_embeddings(tokenized_sentences, max_seq_length):\n",
    "    \"\"\" Pads each sequence to a maximum length \"\"\"\n",
    "    proc_sentences = []\n",
    "    for tokens in tokenized_sentences:\n",
    "        if len(tokens) >= max_seq_length:\n",
    "            proc_sentences.append([[t] for t in tokens[:max_seq_length]])\n",
    "        else:\n",
    "            proc_sentences.append([[t] for t in tokens+['']*(max_seq_length-len(tokens))])\n",
    "            \n",
    "    return proc_sentences\n",
    "\n",
    "# Define sample data\n",
    "data = ['aaaa bb c', 'd eee']\n",
    "# Pad sequences\n",
    "tokenized_sentences = prepare_corpus_for_char_embeddings([d.split() for d in data], 3)\n",
    "print(f\"Padded sequence: {tokenized_sentences}\")\n",
    "\n",
    "# Fit it on a corpus of data\n",
    "vectorization_layer.adapt(tokenized_sentences)\n",
    "\n",
    "# Print data\n",
    "print(f\"Vectorized output: {vectorization_layer(tokenized_sentences)}\")\n",
    "print(f\"Vocabulary: {vectorization_layer.get_vocabulary()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "625215c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd']]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _split_char(token):\n",
    "    return tf.strings.bytes_split(token)\n",
    "\n",
    "_split_char(tf.constant(['abcd']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a661d17d",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "---\n",
    "*Defining token embeddings using convolution*\n",
    "\n",
    "![Token embeddings via convolution](notebook_images/06_14.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da8abc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 40, 1)        0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " text_vectorization_1 (TextVect  (None, 40, 12)      0           ['lambda[0][0]']                 \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 40, 12, 32)   2752        ['text_vectorization_1[0][0]']   \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, 40)          0           ['input_1[0][0]']                \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 40, 12, 1)    161         ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 40, 64)       1512000     ['text_vectorization[0][0]']     \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 40, 12)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 40, 76)       0           ['embedding[0][0]',              \n",
      "                                                                  'lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " simple_rnn (SimpleRNN)         (None, 40, 64)       9024        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 40, 9)        585         ['simple_rnn[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,524,522\n",
      "Trainable params: 1,524,522\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "K.clear_session()\n",
    "max_seq_length = 40\n",
    "max_token_length = 12\n",
    "\n",
    "def get_fitted_token_vectorization_layer(corpus, max_seq_length, vocabulary_size=None):\n",
    "    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n",
    "    \n",
    "    # Define a text vectorization layer\n",
    "    vectorization_layer = TextVectorization(\n",
    "        max_tokens=vocabulary_size, standardize=None,        \n",
    "        output_sequence_length=max_seq_length, \n",
    "    )\n",
    "    # Fit it on a corpus of data\n",
    "    vectorization_layer.adapt(corpus)\n",
    "    \n",
    "    # Get the vocabulary size\n",
    "    n_vocab = len(vectorization_layer.get_vocabulary())\n",
    "\n",
    "    return vectorization_layer, n_vocab\n",
    "\n",
    "\n",
    "def get_fitted_char_vectorization_layer(corpus, max_seq_length, max_token_length, vocabulary_size=None):\n",
    "    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n",
    "    \n",
    "    def _split_char(token):\n",
    "        return tf.strings.bytes_split(token)\n",
    "\n",
    "    # Define a text vectorization layer\n",
    "    vectorization_layer = TextVectorization(\n",
    "        standardize=None,      \n",
    "        split=_split_char,\n",
    "        output_sequence_length=max_token_length, \n",
    "    )\n",
    "\n",
    "    tokenized_sentences = [sent.split() for sent in corpus]\n",
    "    padded_tokenized_sentences = prepare_corpus_for_char_embeddings(tokenized_sentences, max_seq_length)\n",
    "    \n",
    "    # Fit it on a corpus of data\n",
    "    vectorization_layer.adapt(padded_tokenized_sentences)\n",
    "    \n",
    "    # Get the vocabulary size\n",
    "    n_vocab = len(vectorization_layer.get_vocabulary())\n",
    "\n",
    "    return vectorization_layer, n_vocab\n",
    "\n",
    "\n",
    "# Input layer (tokens)\n",
    "word_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "# --------------------- Token based Text Vectorizer + Embeddings ----------------------- #\n",
    "# Text vectorize layer (token)\n",
    "token_vectorize_layer, n_token_vocab = get_fitted_token_vectorization_layer(train_sentences, max_seq_length)\n",
    "\n",
    "# Vectorized output (each word mapped to an int ID)\n",
    "token_vectorized_out = token_vectorize_layer(word_input)\n",
    "\n",
    "# Look up embeddings for the returned IDs\n",
    "token_embedding_out = layers.Embedding(input_dim=n_token_vocab, output_dim=64, mask_zero=True)(token_vectorized_out)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------#\n",
    "\n",
    "# -------------- Char based Text Vectorizer + Convolutional embeddings ----------------- #\n",
    "\n",
    "# Text vectorize layer (char)\n",
    "char_vectorize_layer, n_char_vocab = get_fitted_char_vectorization_layer(train_sentences, max_seq_length, max_token_length)\n",
    "\n",
    "# Vectorized output of each token\n",
    "# Need a [batch size, seq len, 1]\n",
    "# strings.split() returns a RaggedTensor. It needs to be converted to a Tensor. Otherwise the following error will be raised\n",
    "# InvalidArgumentError:  assertion failed: [the given axis (axis = 2) is not squeezable!]\n",
    "#\t [[node model/text_vectorization_1/RaggedSqueeze/Assert/Assert (defined at <ipython-input-26-a2f55ee22434>:17) ]] [Op:__inference_train_function_72435]\n",
    "tokenized_word_input = layers.Lambda(\n",
    "    lambda x: tf.strings.split(x).to_tensor(default_value='', shape=[None, max_seq_length, 1])\n",
    ")(word_input)\n",
    "char_vectorized_out = char_vectorize_layer(tokenized_word_input)\n",
    "\n",
    "\n",
    "# Produces a [batch size, seq length, token_length, emb size]\n",
    "char_embedding_layer = layers.Embedding(input_dim=n_char_vocab, output_dim=32, mask_zero=True)(char_vectorized_out)\n",
    "\n",
    "# A 1D convolutional layer that will generate token embeddings by shifting a convolutional kernel over \n",
    "# the sequence of chars in each token (padded)\n",
    "char_token_output = layers.Conv1D(filters=1, kernel_size=5, strides=1, padding='same', activation='relu')(char_embedding_layer)\n",
    "# There is an additional dimension of size 1 (out channel dimension) that we need to remove\n",
    "char_token_output = layers.Lambda(lambda x: x[:, :, :, 0])(char_token_output)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------#\n",
    "\n",
    "# Concatenate the token and char embeddings\n",
    "concat_embedding_out = layers.Concatenate()([token_embedding_out, char_token_output])\n",
    "\n",
    "# Define a simple RNN layer, it returns an output at each position\n",
    "rnn_layer_1 = layers.SimpleRNN(\n",
    "    units=64, activation='tanh', use_bias=True, return_sequences=True\n",
    ")\n",
    "\n",
    "rnn_out_1 = rnn_layer_1(concat_embedding_out)\n",
    "\n",
    "# Defines the final prediction layer\n",
    "dense_layer = layers.Dense(n_classes, activation='softmax')\n",
    "dense_out = dense_layer(rnn_out_1)\n",
    "\n",
    "# Defines the model\n",
    "char_token_embedding_rnn = tf.keras.Model(inputs=word_input, outputs=dense_out)\n",
    " \n",
    "# Define a macro accuracy measure\n",
    "mean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy, name='macro_accuracy')\n",
    "\n",
    "# Compile the model with a loss optimizer and metrics\n",
    "char_token_embedding_rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[mean_accuracy_metric])\n",
    "\n",
    "# Summary of the model\n",
    "char_token_embedding_rnn.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58320e",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7adc3bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n",
      "Class weights: {1: 0.006811025015037328, 5: 0.16176470588235295, 3: 0.17500000000000002, 0: 0.18272425249169436, 4: 0.25507950530035334, 6: 0.31182505399568033, 2: 0.33595113438045376, 8: 0.9982713915298186, 7: 1.0}\n",
      "Epoch 1/3\n",
      "220/220 [==============================] - 64s 231ms/step - loss: 0.0288 - macro_accuracy: 0.5550 - val_loss: 0.4320 - val_macro_accuracy: 0.7073\n",
      "Epoch 2/3\n",
      "220/220 [==============================] - 48s 220ms/step - loss: 0.0087 - macro_accuracy: 0.8833 - val_loss: 0.1971 - val_macro_accuracy: 0.7977\n",
      "Epoch 3/3\n",
      "220/220 [==============================] - 49s 221ms/step - loss: 0.0030 - macro_accuracy: 0.9576 - val_loss: 0.1046 - val_macro_accuracy: 0.7925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26cc2cea208>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sample_weights_from_class_weights(labels, class_weights):\n",
    "    \"\"\" From the class weights generate sample weights \"\"\"\n",
    "    return np.vectorize(class_weights.get)(labels)\n",
    "\n",
    "train_class_weights = get_class_weights(train_labels)\n",
    "print(f\"Class weights: {train_class_weights}\")\n",
    "\n",
    "# Make train_sequences an array\n",
    "train_sentences = np.array(train_sentences)\n",
    "# Get sample weights (we cannot use class_weight with TextVectorization layer)\n",
    "train_sample_weights = get_sample_weights_from_class_weights(padded_train_labels, train_class_weights)\n",
    "\n",
    "# Training the model\n",
    "char_token_embedding_rnn.fit(\n",
    "    train_sentences, padded_train_labels,\n",
    "    sample_weight=train_sample_weights,\n",
    "    batch_size=64,\n",
    "    epochs=3, \n",
    "    validation_data=(np.array(valid_sentences), padded_valid_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fda96a",
   "metadata": {},
   "source": [
    "## Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cd09441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 4s 38ms/step - loss: 0.1151 - macro_accuracy: 0.7597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1150645762681961, 0.7596610188484192]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_token_embedding_rnn.evaluate(np.array(test_sentences), padded_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b1bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
