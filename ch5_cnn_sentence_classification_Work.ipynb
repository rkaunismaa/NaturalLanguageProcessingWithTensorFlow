{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run --gpus all -it -v $(realpath ~/):/tf/All -v /home/rob/Data2:/home/rob/Data2 --env HF_DATASETS_CACHE=/home/rob/Data2/huggingface/datasets --env TRANSFORMERS_CACHE=/home/rob/Data2/huggingface/transformers -p 8888:8888 -p 6006:6006 d139afc9cfb2\n",
    "\n",
    "# 2nd pass ...\n",
    "# Run Date: Thursday, January 19, 2023\n",
    "# Run Time: 00:00:25\n",
    "\n",
    "# First Pass ...\n",
    "# Run Date: Thursday, January 19, 2023\n",
    "# Run Time: 00:00:26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import date\n",
    "\n",
    "startTime = time.time()\n",
    "todaysDate = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classification with Convolution Neural Networks\n",
    "[Paper](https://arxiv.org/pdf/1408.5882.pdf): Convolutional Neural Networks for Sentence Classification by Yoon Kim\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/packt_nlp_tensorflow_2/blob/master/Ch05-Sentence-Classification/ch5_cnn_sentence_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 17:29:47.275172: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 17:29:47.832756: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-19 17:29:47.832805: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-19 17:29:47.832812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 54321\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Checking the Dataset\n",
    "This [dataset](http://cogcomp.cs.illinois.edu/Data/QA/QC/) is composed of questions as inputs and their respective type as the output. For example, (e.g. Who was Abraham Lincon?) and the output or label would be Human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified data/train_5500.label\n",
      "Found and verified data/TREC_10.label\n"
     ]
    }
   ],
   "source": [
    "url = 'http://cogcomp.org/Data/QA/QC/'\n",
    "dir_name = 'data'\n",
    "\n",
    "def download_data(dir_name, filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  \n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    if not os.path.exists(os.path.join(dir_name,filename)):\n",
    "        filepath, _ = urlretrieve(url + filename, os.path.join(dir_name,filename))\n",
    "    else:\n",
    "        filepath = os.path.join(dir_name, filename)\n",
    "    \n",
    "    statinfo = os.stat(filepath)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filepath)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filepath + '. Can you get to it with a browser?')\n",
    "        \n",
    "    return filepath\n",
    "\n",
    "train_filename = download_data(dir_name, 'train_5500.label', 335858)\n",
    "test_filename = download_data(dir_name, 'TREC_10.label',23354)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data\n",
    "Below we load the text into the program and do some simple preprocessing on data. For each example, we obtain,\n",
    "\n",
    "* Question\n",
    "* Category\n",
    "* Sub-category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_questions has 5452 questions / 5452 labels\n",
      "Some samples\n",
      "\thow did serfdom develop in and then leave russia ? / cat - DESC / sub_cat - manner\n",
      "\twhat films featured the character popeye doyle ? / cat - ENTY / sub_cat - cremat\n",
      "\thow can i find a list of celebrities ' real names ? / cat - DESC / sub_cat - manner\n",
      "\twhat fowl grabs the spotlight after the chinese year of the monkey ? / cat - ENTY / sub_cat - animal\n",
      "\twhat is the full form of .com ? / cat - ABBR / sub_cat - exp\n",
      "\twhat contemptible scoundrel stole the cork from my lunch ? / cat - HUM / sub_cat - ind\n",
      "\twhat team did baseball 's st. louis browns become ? / cat - HUM / sub_cat - gr\n",
      "\twhat is the oldest profession ? / cat - HUM / sub_cat - title\n",
      "\twhat are liver enzymes ? / cat - DESC / sub_cat - def\n",
      "\tname the scar-faced bounty hunter of the old west . / cat - HUM / sub_cat - ind\n",
      "\n",
      "test_questions has 500 questions / 500 labels\n",
      "Some samples\n",
      "\thow far is it from denver to aspen ? / cat - NUM / sub_cat - dist\n",
      "\twhat county is modesto , california in ? / cat - LOC / sub_cat - city\n",
      "\twho was galileo ? / cat - HUM / sub_cat - desc\n",
      "\twhat is an atom ? / cat - DESC / sub_cat - def\n",
      "\twhen did hawaii become a state ? / cat - NUM / sub_cat - date\n",
      "\thow tall is the sears building ? / cat - NUM / sub_cat - dist\n",
      "\tgeorge bush purchased a small interest in which baseball team ? / cat - HUM / sub_cat - gr\n",
      "\twhat is australia 's national flower ? / cat - ENTY / sub_cat - plant\n",
      "\twhy does the moon turn orange ? / cat - DESC / sub_cat - reason\n",
      "\twhat is autism ? / cat - DESC / sub_cat - def\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    '''\n",
    "    Read data from a file with given filename\n",
    "    Returns a list of strings where each string is a lower case word\n",
    "    '''\n",
    "\n",
    "    # Holds question strings, categories and sub categories\n",
    "    # category/sub_cateory definitions: https://cogcomp.seas.upenn.edu/Data/QA/QC/definition.html\n",
    "    questions, categories, sub_categories = [], [], []     \n",
    "    \n",
    "    with open(filename,'r',encoding='latin-1') as f:        \n",
    "        # Read each line\n",
    "        for row in f:   \n",
    "            # Each string has format <cat>:<sub cat> <question>\n",
    "            # Split by : to separate cat and (sub_cat + question)\n",
    "            row_str = row.split(\":\")        \n",
    "            cat, sub_cat_and_question = row_str[0], row_str[1]\n",
    "            tokens = sub_cat_and_question.split(' ')\n",
    "            # The first word in sub_cat_and_question is the sub category\n",
    "            # rest is the question\n",
    "            sub_cat, question = tokens[0], ' '.join(tokens[1:])        \n",
    "            \n",
    "            questions.append(question.lower().strip())\n",
    "            categories.append(cat)\n",
    "            sub_categories.append(sub_cat)\n",
    "            \n",
    "\n",
    "    return questions, categories, sub_categories\n",
    "\n",
    "train_questions, train_categories, train_sub_categories = read_data(train_filename)\n",
    "test_questions, test_categories, test_sub_categories = read_data(test_filename)\n",
    "\n",
    "n_samples = 10\n",
    "print(f\"train_questions has {len(train_questions)} questions / {len(train_categories)} labels\")\n",
    "print(\"Some samples\")\n",
    "for question, cat, sub_cat in zip(train_questions[:n_samples], train_categories[:n_samples], train_sub_categories[:n_samples]):    \n",
    "    print(f\"\\t{question} / cat - {cat} / sub_cat - {sub_cat}\")\n",
    "          \n",
    "print(f\"\\ntest_questions has {len(test_questions)} questions / {len(test_categories)} labels\")\n",
    "print(\"Some samples\")\n",
    "for question, cat, sub_cat in zip(test_questions[:n_samples], test_categories[:n_samples], test_sub_categories[:n_samples]):    \n",
    "    print(f\"\\t{question} / cat - {cat} / sub_cat - {sub_cat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert train/test data to `pd.DataFrame`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how did serfdom develop in and then leave russ...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what films featured the character popeye doyle ?</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>cremat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how can i find a list of celebrities ' real na...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what fowl grabs the spotlight after the chines...</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is the full form of .com ?</td>\n",
       "      <td>ABBR</td>\n",
       "      <td>exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>what contemptible scoundrel stole the cork fro...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>what team did baseball 's st. louis browns bec...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>gr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what is the oldest profession ?</td>\n",
       "      <td>HUM</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what are liver enzymes ?</td>\n",
       "      <td>DESC</td>\n",
       "      <td>def</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>name the scar-faced bounty hunter of the old w...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>ind</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question category sub_category\n",
       "0  how did serfdom develop in and then leave russ...     DESC       manner\n",
       "1   what films featured the character popeye doyle ?     ENTY       cremat\n",
       "2  how can i find a list of celebrities ' real na...     DESC       manner\n",
       "3  what fowl grabs the spotlight after the chines...     ENTY       animal\n",
       "4                    what is the full form of .com ?     ABBR          exp\n",
       "5  what contemptible scoundrel stole the cork fro...      HUM          ind\n",
       "6  what team did baseball 's st. louis browns bec...      HUM           gr\n",
       "7                    what is the oldest profession ?      HUM        title\n",
       "8                           what are liver enzymes ?     DESC          def\n",
       "9  name the scar-faced bounty hunter of the old w...      HUM          ind"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training and testing\n",
    "train_df = pd.DataFrame(\n",
    "    {'question': train_questions, 'category': train_categories, 'sub_category': train_sub_categories}\n",
    ")\n",
    "test_df = pd.DataFrame(\n",
    "    {'question': test_questions, 'category': test_categories, 'sub_category': test_sub_categories}\n",
    ")\n",
    "\n",
    "train_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data for better randomization\n",
    "train_df = train_df.sample(frac=1.0, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert string labels to integer IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label->ID mapping: {'DESC': 0, 'ENTY': 1, 'LOC': 2, 'NUM': 3, 'HUM': 4, 'ABBR': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5267</th>\n",
       "      <td>what is an aurora ?</td>\n",
       "      <td>0</td>\n",
       "      <td>def</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>what articles of clothing are tokens in monopo...</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>what causes rust ?</td>\n",
       "      <td>0</td>\n",
       "      <td>reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>what does an irate car owner call iron oxide ?</td>\n",
       "      <td>1</td>\n",
       "      <td>termeq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>what do we call the imaginary line along the t...</td>\n",
       "      <td>2</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3631</th>\n",
       "      <td>why is hockey so violent ?</td>\n",
       "      <td>0</td>\n",
       "      <td>reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>how many characters makes up a word for typing...</td>\n",
       "      <td>3</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>what peter blatty novel recounts the horrors o...</td>\n",
       "      <td>1</td>\n",
       "      <td>cremat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>what is measured in curies ?</td>\n",
       "      <td>0</td>\n",
       "      <td>def</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4472</th>\n",
       "      <td>what does seccession mean ?</td>\n",
       "      <td>0</td>\n",
       "      <td>def</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  category sub_category\n",
       "5267                                what is an aurora ?         0          def\n",
       "21    what articles of clothing are tokens in monopo...         1        other\n",
       "3258                                 what causes rust ?         0       reason\n",
       "1356     what does an irate car owner call iron oxide ?         1       termeq\n",
       "1529  what do we call the imaginary line along the t...         2        other\n",
       "3631                         why is hockey so violent ?         0       reason\n",
       "4802  how many characters makes up a word for typing...         3        count\n",
       "2288  what peter blatty novel recounts the horrors o...         1       cremat\n",
       "803                        what is measured in curies ?         0          def\n",
       "4472                        what does seccession mean ?         0          def"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the label to ID mapping\n",
    "unique_cats = train_df[\"category\"].unique()\n",
    "labels_map = dict(zip(unique_cats, np.arange(unique_cats.shape[0])))\n",
    "print(f\"Label->ID mapping: {labels_map}\")\n",
    "\n",
    "n_classes = len(labels_map)\n",
    "\n",
    "# Convert all string labels to IDs\n",
    "train_df[\"category\"] = train_df[\"category\"].map(labels_map)\n",
    "test_df[\"category\"] = test_df[\"category\"].map(labels_map)\n",
    "\n",
    "# View some data\n",
    "train_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data to train and valid subsets\n",
    "\n",
    "Here we split the training data (90%) and validation data (10%) from the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (4906, 3)\n",
      "Valid size: (546, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4622</th>\n",
       "      <td>what is object-oriented design ?</td>\n",
       "      <td>0</td>\n",
       "      <td>def</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3179</th>\n",
       "      <td>what colors make up a rainbow ?</td>\n",
       "      <td>1</td>\n",
       "      <td>color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>name the two mystical ravens odin has at his c...</td>\n",
       "      <td>1</td>\n",
       "      <td>animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>what historical event happened in dogtown in 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>what italian city of 155 were leonardo da vinc...</td>\n",
       "      <td>2</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  category sub_category\n",
       "4622                   what is object-oriented design ?         0          def\n",
       "3179                    what colors make up a rainbow ?         1        color\n",
       "1500  name the two mystical ravens odin has at his c...         1       animal\n",
       "775   what historical event happened in dogtown in 1...         1        event\n",
       "3274  what italian city of 155 were leonardo da vinc...         2         city"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.1)\n",
    "print(f\"Train size: {train_df.shape}\")\n",
    "print(f\"Valid size: {valid_df.shape}\")\n",
    "\n",
    "# Print data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "Here we define a tokenizer, that is trained on training data. Finally we will find the vocabulary size by checking the size of the `index_word` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabluary size: 7916\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a tokenizer and fit on train data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df[\"question\"].tolist())\n",
    "\n",
    "# Derive the vocabulary size\n",
    "n_vocab = len(tokenizer.index_word) + 1\n",
    "print(f\"Vocabluary size: {n_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the sequence length\n",
    "\n",
    "Here we analyze the `1%` and `99%` percentiles of the sequence lengths. We will use the `99%` percentile as our maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4906.000000\n",
       "mean       10.061761\n",
       "std         3.777510\n",
       "min         2.000000\n",
       "1%          4.000000\n",
       "50%         9.000000\n",
       "99%        22.000000\n",
       "max        37.000000\n",
       "Name: question, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split each string by \" \", compute length of the list, get the percentiles\n",
    "train_df[\"question\"].str.split(\" \").str.len().describe(percentiles=[0.01, 0.5, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Shorter Sentences\n",
    "We use padding to pad short sentences so that all the sentences are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each list of tokens to a list of IDs, using tokenizer's mapping\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df[\"question\"].tolist())\n",
    "train_labels = train_df[\"category\"].values\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_df[\"question\"].tolist())\n",
    "valid_labels = valid_df[\"category\"].values\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df[\"question\"].tolist())\n",
    "test_labels = test_df[\"category\"].values\n",
    "\n",
    "max_seq_length = 22\n",
    "\n",
    "# Pad shorter sentences and truncate longer ones (maximum length: max_seq_length)\n",
    "preprocessed_train_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    train_sequences, maxlen=max_seq_length, padding='post', truncating='post'\n",
    ")\n",
    "preprocessed_valid_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    valid_sequences, maxlen=max_seq_length, padding='post', truncating='post'\n",
    ")\n",
    "preprocessed_test_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    test_sequences, maxlen=max_seq_length, padding='post', truncating='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Classifying Convolution Neural Network\n",
    "We are going to implement a very simple CNN to classify sentences. However you will see that even with this simple structure we achieve good accuracies. Our CNN will have one layer (with 3 different parallel layers). This will be followed by a pooling-over-time layer and finally a fully connected layer that produces the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 22)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 22, 64)       506624      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 22, 100)      19300       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 22, 100)      25700       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 22, 100)      32100       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 22, 300)      0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_1[0][0]',               \n",
      "                                                                  'conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 1, 300)       0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 300)          0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 6)            1806        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 585,530\n",
      "Trainable params: 585,530\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 17:29:48.829803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:29:48.829992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:29:48.834261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:29:48.834430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:29:48.834581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:29:48.834726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:29:48.834837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2006] Ignoring visible gpu device (device: 1, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1) with core count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n",
      "2023-01-19 17:29:48.835111: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 17:29:48.835473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:29:48.835632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:29:48.835778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:29:49.258871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:29:49.259065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:29:49.259208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:29:49.259313: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-01-19 17:29:49.259334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6647 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Input layer takes word IDs as inputs\n",
    "word_id_inputs = layers.Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "# Get the embeddings of the inputs / out [batch_size, sent_length, output_dim]\n",
    "embedding_out = layers.Embedding(input_dim=n_vocab, output_dim=64)(word_id_inputs)\n",
    "\n",
    "\n",
    "# For all layers: in [batch_size, sent_length, emb_size] / out [batch_size, sent_length, 100]\n",
    "conv1_1 = layers.Conv1D(\n",
    "    100, kernel_size=3, strides=1, padding='same', activation='relu'\n",
    ")(embedding_out)\n",
    "conv1_2 = layers.Conv1D(\n",
    "    100, kernel_size=4, strides=1, padding='same', activation='relu'\n",
    ")(embedding_out)\n",
    "conv1_3 = layers.Conv1D(\n",
    "    100, kernel_size=5, strides=1, padding='same', activation='relu'\n",
    ")(embedding_out)\n",
    "\n",
    "# in previous conve outputs / out [batch_size, sent_length, 300]\n",
    "conv_out = layers.Concatenate(axis=-1)([conv1_1, conv1_2, conv1_3])\n",
    "\n",
    "# Pooling over time operation. This is doing the max pooling over sequence lenth\n",
    "# in other words, each feature map results in a single output\n",
    "# in [batch_size, sent_length, 300] / out [batch_size, 1, 300]\n",
    "pool_over_time_out = layers.MaxPool1D(pool_size=max_seq_length, padding='valid')(conv_out)\n",
    "\n",
    "# Flatten the unit length dimension\n",
    "flatten_out = layers.Flatten()(pool_over_time_out)\n",
    "\n",
    "# Compute the final output\n",
    "out = layers.Dense(\n",
    "    n_classes, activation='softmax',\n",
    "    kernel_regularizer=regularizers.l2(0.001)\n",
    ")(flatten_out)\n",
    "\n",
    "# Define the model\n",
    "cnn_model = Model(inputs=word_id_inputs, outputs=out)\n",
    "\n",
    "# Compile the model with loss/optimzier/metrics\n",
    "cnn_model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Here we train the model with a pre-defined batch size for a number of epochs. We will use a built-in callback.\n",
    "\n",
    "* `ReduceLROnPlateau` - Reduces the learning rate when no improvement detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 17:29:50.385911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8101\n",
      "2023-01-19 17:29:51.072652: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1f4c9dc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-19 17:29:51.072682: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2070 SUPER, Compute Capability 7.5\n",
      "2023-01-19 17:29:51.076716: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-01-19 17:29:51.169786: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 7s 115ms/step - loss: 1.6244 - accuracy: 0.3936 - val_loss: 1.4413 - val_accuracy: 0.5495 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "39/39 [==============================] - 3s 76ms/step - loss: 1.0795 - accuracy: 0.6763 - val_loss: 0.8772 - val_accuracy: 0.7070 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "39/39 [==============================] - 2s 39ms/step - loss: 0.5904 - accuracy: 0.8286 - val_loss: 0.6210 - val_accuracy: 0.8022 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "39/39 [==============================] - 1s 36ms/step - loss: 0.3238 - accuracy: 0.9291 - val_loss: 0.5028 - val_accuracy: 0.8168 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.1834 - accuracy: 0.9672 - val_loss: 0.4626 - val_accuracy: 0.8535 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.1158 - accuracy: 0.9829 - val_loss: 0.4531 - val_accuracy: 0.8553 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0811 - accuracy: 0.9916 - val_loss: 0.4553 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.0639 - accuracy: 0.9974 - val_loss: 0.4589 - val_accuracy: 0.8608 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9992\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.0543 - accuracy: 0.9992 - val_loss: 0.4637 - val_accuracy: 0.8626 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0499 - accuracy: 0.9994 - val_loss: 0.4637 - val_accuracy: 0.8626 - lr: 1.0000e-04\n",
      "Epoch 11/25\n",
      "39/39 [==============================] - 0s 4ms/step - loss: 0.0494 - accuracy: 0.9996 - val_loss: 0.4640 - val_accuracy: 0.8626 - lr: 1.0000e-04\n",
      "Epoch 12/25\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9998\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0489 - accuracy: 0.9998 - val_loss: 0.4647 - val_accuracy: 0.8608 - lr: 1.0000e-04\n",
      "Epoch 13/25\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0485 - accuracy: 0.9998 - val_loss: 0.4647 - val_accuracy: 0.8608 - lr: 1.0000e-05\n",
      "Epoch 14/25\n",
      "39/39 [==============================] - 0s 7ms/step - loss: 0.0485 - accuracy: 0.9998 - val_loss: 0.4648 - val_accuracy: 0.8608 - lr: 1.0000e-05\n",
      "Epoch 15/25\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 0.0487 - accuracy: 0.9997\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "39/39 [==============================] - 0s 7ms/step - loss: 0.0484 - accuracy: 0.9998 - val_loss: 0.4649 - val_accuracy: 0.8608 - lr: 1.0000e-05\n",
      "Epoch 16/25\n",
      "39/39 [==============================] - 0s 7ms/step - loss: 0.0484 - accuracy: 0.9998 - val_loss: 0.4649 - val_accuracy: 0.8608 - lr: 1.0000e-06\n",
      "Epoch 17/25\n",
      "39/39 [==============================] - 0s 7ms/step - loss: 0.0484 - accuracy: 0.9998 - val_loss: 0.4649 - val_accuracy: 0.8608 - lr: 1.0000e-06\n",
      "Epoch 18/25\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9998\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0484 - accuracy: 0.9998 - val_loss: 0.4649 - val_accuracy: 0.8608 - lr: 1.0000e-06\n",
      "Epoch 19/25\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0484 - accuracy: 0.9998 - val_loss: 0.4649 - val_accuracy: 0.8608 - lr: 1.0000e-06\n",
      "Epoch 20/25\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0484 - accuracy: 0.9998 - val_loss: 0.4649 - val_accuracy: 0.8608 - lr: 1.0000e-06\n",
      "Epoch 21/25\n",
      "39/39 [==============================] - 0s 7ms/step - loss: 0.0484 - accuracy: 0.9998 - val_loss: 0.4649 - val_accuracy: 0.8608 - lr: 1.0000e-06\n",
      "Epoch 22/25\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 0.0484 - accuracy: 0.9998 - val_loss: 0.4650 - val_accuracy: 0.8608 - lr: 1.0000e-06\n",
      "Epoch 23/25\n",
      "39/39 [==============================] - 0s 7ms/step - loss: 0.0484 - accuracy: 0.9998 - val_loss: 0.4650 - val_accuracy: 0.8608 - lr: 1.0000e-06\n",
      "Epoch 24/25\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0483 - accuracy: 0.9998 - val_loss: 0.4650 - val_accuracy: 0.8608 - lr: 1.0000e-06\n",
      "Epoch 25/25\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 0.0483 - accuracy: 0.9998 - val_loss: 0.4650 - val_accuracy: 0.8608 - lr: 1.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc3324554f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call backs\n",
    "lr_reduce_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=3, verbose=1,\n",
    "    mode='auto', min_delta=0.0001, min_lr=0.000001\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(\n",
    "    preprocessed_train_sequences, train_labels, \n",
    "    validation_data=(preprocessed_valid_sequences, valid_labels),\n",
    "    batch_size=128, \n",
    "    epochs=25,\n",
    "    callbacks=[lr_reduce_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.3697260320186615, 'accuracy': 0.8939999938011169}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.evaluate(preprocessed_test_sequences, test_labels, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Run Date: Thursday, January 19, 2023\n",
      "# Run Time: 00:00:25\n"
     ]
    }
   ],
   "source": [
    "endTime = time.time()\n",
    "elapsedTime = time.strftime(\"%H:%M:%S\", time.gmtime(endTime - startTime))\n",
    "\n",
    "print(todaysDate.strftime('# Run Date: %A, %B %d, %Y'))\n",
    "print(f\"# Run Time: {elapsedTime}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
