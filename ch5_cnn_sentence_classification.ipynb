{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classification with Convolution Neural Networks\n",
    "[Paper](https://arxiv.org/pdf/1408.5882.pdf): Convolutional Neural Networks for Sentence Classification by Yoon Kim\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/packt_nlp_tensorflow_2/blob/master/Ch05-Sentence-Classification/ch5_cnn_sentence_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 12:17:35.589954: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 12:17:36.122712: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-19 12:17:36.122788: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-19 12:17:36.122795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 54321\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Checking the Dataset\n",
    "This [dataset](http://cogcomp.cs.illinois.edu/Data/QA/QC/) is composed of questions as inputs and their respective type as the output. For example, (e.g. Who was Abraham Lincon?) and the output or label would be Human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified data\\train_5500.label\n",
      "Found and verified data\\TREC_10.label\n"
     ]
    }
   ],
   "source": [
    "url = 'http://cogcomp.org/Data/QA/QC/'\n",
    "dir_name = 'data'\n",
    "\n",
    "def download_data(dir_name, filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  \n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    if not os.path.exists(os.path.join(dir_name,filename)):\n",
    "        filepath, _ = urlretrieve(url + filename, os.path.join(dir_name,filename))\n",
    "    else:\n",
    "        filepath = os.path.join(dir_name, filename)\n",
    "    \n",
    "    statinfo = os.stat(filepath)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filepath)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filepath + '. Can you get to it with a browser?')\n",
    "        \n",
    "    return filepath\n",
    "\n",
    "train_filename = download_data(dir_name, 'train_5500.label', 335858)\n",
    "test_filename = download_data(dir_name, 'TREC_10.label',23354)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data\n",
    "Below we load the text into the program and do some simple preprocessing on data. For each example, we obtain,\n",
    "\n",
    "* Question\n",
    "* Category\n",
    "* Sub-category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_questions has 5452 questions / 5452 labels\n",
      "Some samples\n",
      "\thow did serfdom develop in and then leave russia ? / cat - DESC / sub_cat - manner\n",
      "\twhat films featured the character popeye doyle ? / cat - ENTY / sub_cat - cremat\n",
      "\thow can i find a list of celebrities ' real names ? / cat - DESC / sub_cat - manner\n",
      "\twhat fowl grabs the spotlight after the chinese year of the monkey ? / cat - ENTY / sub_cat - animal\n",
      "\twhat is the full form of .com ? / cat - ABBR / sub_cat - exp\n",
      "\twhat contemptible scoundrel stole the cork from my lunch ? / cat - HUM / sub_cat - ind\n",
      "\twhat team did baseball 's st. louis browns become ? / cat - HUM / sub_cat - gr\n",
      "\twhat is the oldest profession ? / cat - HUM / sub_cat - title\n",
      "\twhat are liver enzymes ? / cat - DESC / sub_cat - def\n",
      "\tname the scar-faced bounty hunter of the old west . / cat - HUM / sub_cat - ind\n",
      "\n",
      "test_questions has 500 questions / 500 labels\n",
      "Some samples\n",
      "\thow far is it from denver to aspen ? / cat - NUM / sub_cat - dist\n",
      "\twhat county is modesto , california in ? / cat - LOC / sub_cat - city\n",
      "\twho was galileo ? / cat - HUM / sub_cat - desc\n",
      "\twhat is an atom ? / cat - DESC / sub_cat - def\n",
      "\twhen did hawaii become a state ? / cat - NUM / sub_cat - date\n",
      "\thow tall is the sears building ? / cat - NUM / sub_cat - dist\n",
      "\tgeorge bush purchased a small interest in which baseball team ? / cat - HUM / sub_cat - gr\n",
      "\twhat is australia 's national flower ? / cat - ENTY / sub_cat - plant\n",
      "\twhy does the moon turn orange ? / cat - DESC / sub_cat - reason\n",
      "\twhat is autism ? / cat - DESC / sub_cat - def\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    '''\n",
    "    Read data from a file with given filename\n",
    "    Returns a list of strings where each string is a lower case word\n",
    "    '''\n",
    "\n",
    "    # Holds question strings, categories and sub categories\n",
    "    # category/sub_cateory definitions: https://cogcomp.seas.upenn.edu/Data/QA/QC/definition.html\n",
    "    questions, categories, sub_categories = [], [], []     \n",
    "    \n",
    "    with open(filename,'r',encoding='latin-1') as f:        \n",
    "        # Read each line\n",
    "        for row in f:   \n",
    "            # Each string has format <cat>:<sub cat> <question>\n",
    "            # Split by : to separate cat and (sub_cat + question)\n",
    "            row_str = row.split(\":\")        \n",
    "            cat, sub_cat_and_question = row_str[0], row_str[1]\n",
    "            tokens = sub_cat_and_question.split(' ')\n",
    "            # The first word in sub_cat_and_question is the sub category\n",
    "            # rest is the question\n",
    "            sub_cat, question = tokens[0], ' '.join(tokens[1:])        \n",
    "            \n",
    "            questions.append(question.lower().strip())\n",
    "            categories.append(cat)\n",
    "            sub_categories.append(sub_cat)\n",
    "            \n",
    "\n",
    "    return questions, categories, sub_categories\n",
    "\n",
    "train_questions, train_categories, train_sub_categories = read_data(train_filename)\n",
    "test_questions, test_categories, test_sub_categories = read_data(test_filename)\n",
    "\n",
    "n_samples = 10\n",
    "print(f\"train_questions has {len(train_questions)} questions / {len(train_categories)} labels\")\n",
    "print(\"Some samples\")\n",
    "for question, cat, sub_cat in zip(train_questions[:n_samples], train_categories[:n_samples], train_sub_categories[:n_samples]):    \n",
    "    print(f\"\\t{question} / cat - {cat} / sub_cat - {sub_cat}\")\n",
    "          \n",
    "print(f\"\\ntest_questions has {len(test_questions)} questions / {len(test_categories)} labels\")\n",
    "print(\"Some samples\")\n",
    "for question, cat, sub_cat in zip(test_questions[:n_samples], test_categories[:n_samples], test_sub_categories[:n_samples]):    \n",
    "    print(f\"\\t{question} / cat - {cat} / sub_cat - {sub_cat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert train/test data to `pd.DataFrame`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how did serfdom develop in and then leave russ...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what films featured the character popeye doyle ?</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>cremat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how can i find a list of celebrities ' real na...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what fowl grabs the spotlight after the chines...</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is the full form of .com ?</td>\n",
       "      <td>ABBR</td>\n",
       "      <td>exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>what contemptible scoundrel stole the cork fro...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>what team did baseball 's st. louis browns bec...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>gr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what is the oldest profession ?</td>\n",
       "      <td>HUM</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what are liver enzymes ?</td>\n",
       "      <td>DESC</td>\n",
       "      <td>def</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>name the scar-faced bounty hunter of the old w...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>ind</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question category sub_category\n",
       "0  how did serfdom develop in and then leave russ...     DESC       manner\n",
       "1   what films featured the character popeye doyle ?     ENTY       cremat\n",
       "2  how can i find a list of celebrities ' real na...     DESC       manner\n",
       "3  what fowl grabs the spotlight after the chines...     ENTY       animal\n",
       "4                    what is the full form of .com ?     ABBR          exp\n",
       "5  what contemptible scoundrel stole the cork fro...      HUM          ind\n",
       "6  what team did baseball 's st. louis browns bec...      HUM           gr\n",
       "7                    what is the oldest profession ?      HUM        title\n",
       "8                           what are liver enzymes ?     DESC          def\n",
       "9  name the scar-faced bounty hunter of the old w...      HUM          ind"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training and testing\n",
    "train_df = pd.DataFrame(\n",
    "    {'question': train_questions, 'category': train_categories, 'sub_category': train_sub_categories}\n",
    ")\n",
    "test_df = pd.DataFrame(\n",
    "    {'question': test_questions, 'category': test_categories, 'sub_category': test_sub_categories}\n",
    ")\n",
    "\n",
    "train_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data for better randomization\n",
    "train_df = train_df.sample(frac=1.0, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert string labels to integer IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label->ID mapping: {'DESC': 0, 'ENTY': 1, 'LOC': 2, 'NUM': 3, 'HUM': 4, 'ABBR': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5267</th>\n",
       "      <td>what is an aurora ?</td>\n",
       "      <td>0</td>\n",
       "      <td>def</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>what articles of clothing are tokens in monopo...</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>what causes rust ?</td>\n",
       "      <td>0</td>\n",
       "      <td>reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>what does an irate car owner call iron oxide ?</td>\n",
       "      <td>1</td>\n",
       "      <td>termeq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>what do we call the imaginary line along the t...</td>\n",
       "      <td>2</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3631</th>\n",
       "      <td>why is hockey so violent ?</td>\n",
       "      <td>0</td>\n",
       "      <td>reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>how many characters makes up a word for typing...</td>\n",
       "      <td>3</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>what peter blatty novel recounts the horrors o...</td>\n",
       "      <td>1</td>\n",
       "      <td>cremat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>what is measured in curies ?</td>\n",
       "      <td>0</td>\n",
       "      <td>def</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4472</th>\n",
       "      <td>what does seccession mean ?</td>\n",
       "      <td>0</td>\n",
       "      <td>def</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  category sub_category\n",
       "5267                                what is an aurora ?         0          def\n",
       "21    what articles of clothing are tokens in monopo...         1        other\n",
       "3258                                 what causes rust ?         0       reason\n",
       "1356     what does an irate car owner call iron oxide ?         1       termeq\n",
       "1529  what do we call the imaginary line along the t...         2        other\n",
       "3631                         why is hockey so violent ?         0       reason\n",
       "4802  how many characters makes up a word for typing...         3        count\n",
       "2288  what peter blatty novel recounts the horrors o...         1       cremat\n",
       "803                        what is measured in curies ?         0          def\n",
       "4472                        what does seccession mean ?         0          def"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the label to ID mapping\n",
    "unique_cats = train_df[\"category\"].unique()\n",
    "labels_map = dict(zip(unique_cats, np.arange(unique_cats.shape[0])))\n",
    "print(f\"Label->ID mapping: {labels_map}\")\n",
    "\n",
    "n_classes = len(labels_map)\n",
    "\n",
    "# Convert all string labels to IDs\n",
    "train_df[\"category\"] = train_df[\"category\"].map(labels_map)\n",
    "test_df[\"category\"] = test_df[\"category\"].map(labels_map)\n",
    "\n",
    "# View some data\n",
    "train_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data to train and valid subsets\n",
    "\n",
    "Here we split the training data (90%) and validation data (10%) from the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (4906, 3)\n",
      "Valid size: (546, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>who created the monster in mary shelley 's nov...</td>\n",
       "      <td>4</td>\n",
       "      <td>ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>what was the former residence of scottish king...</td>\n",
       "      <td>2</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>who produces spumante ?</td>\n",
       "      <td>4</td>\n",
       "      <td>gr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>what was the bridge of san luis rey made of ?</td>\n",
       "      <td>1</td>\n",
       "      <td>substance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>how many bends are there in a standard paper c...</td>\n",
       "      <td>3</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  category sub_category\n",
       "3009  who created the monster in mary shelley 's nov...         4          ind\n",
       "2928  what was the former residence of scottish king...         2        other\n",
       "148                             who produces spumante ?         4           gr\n",
       "512       what was the bridge of san luis rey made of ?         1    substance\n",
       "152   how many bends are there in a standard paper c...         3        count"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.1)\n",
    "print(f\"Train size: {train_df.shape}\")\n",
    "print(f\"Valid size: {valid_df.shape}\")\n",
    "\n",
    "# Print data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "Here we define a tokenizer, that is trained on training data. Finally we will find the vocabulary size by checking the size of the `index_word` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabluary size: 7871\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a tokenizer and fit on train data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df[\"question\"].tolist())\n",
    "\n",
    "# Derive the vocabulary size\n",
    "n_vocab = len(tokenizer.index_word) + 1\n",
    "print(f\"Vocabluary size: {n_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the sequence length\n",
    "\n",
    "Here we analyze the `1%` and `99%` percentiles of the sequence lengths. We will use the `99%` percentile as our maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4906.000000\n",
       "mean       10.067265\n",
       "std         3.790105\n",
       "min         2.000000\n",
       "1%          4.000000\n",
       "50%        10.000000\n",
       "99%        22.000000\n",
       "max        37.000000\n",
       "Name: question, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split each string by \" \", compute length of the list, get the percentiles\n",
    "train_df[\"question\"].str.split(\" \").str.len().describe(percentiles=[0.01, 0.5, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Shorter Sentences\n",
    "We use padding to pad short sentences so that all the sentences are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each list of tokens to a list of IDs, using tokenizer's mapping\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df[\"question\"].tolist())\n",
    "train_labels = train_df[\"category\"].values\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_df[\"question\"].tolist())\n",
    "valid_labels = valid_df[\"category\"].values\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df[\"question\"].tolist())\n",
    "test_labels = test_df[\"category\"].values\n",
    "\n",
    "max_seq_length = 22\n",
    "\n",
    "# Pad shorter sentences and truncate longer ones (maximum length: max_seq_length)\n",
    "preprocessed_train_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    train_sequences, maxlen=max_seq_length, padding='post', truncating='post'\n",
    ")\n",
    "preprocessed_valid_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    valid_sequences, maxlen=max_seq_length, padding='post', truncating='post'\n",
    ")\n",
    "preprocessed_test_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    test_sequences, maxlen=max_seq_length, padding='post', truncating='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Classifying Convolution Neural Network\n",
    "We are going to implement a very simple CNN to classify sentences. However you will see that even with this simple structure we achieve good accuracies. Our CNN will have one layer (with 3 different parallel layers). This will be followed by a pooling-over-time layer and finally a fully connected layer that produces the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 22)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 22, 64)       503744      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 22, 100)      19300       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 22, 100)      25700       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 22, 100)      32100       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 22, 300)      0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_1[0][0]',               \n",
      "                                                                  'conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 1, 300)       0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 300)          0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 6)            1806        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 582,650\n",
      "Trainable params: 582,650\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Input layer takes word IDs as inputs\n",
    "word_id_inputs = layers.Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "# Get the embeddings of the inputs / out [batch_size, sent_length, output_dim]\n",
    "embedding_out = layers.Embedding(input_dim=n_vocab, output_dim=64)(word_id_inputs)\n",
    "\n",
    "\n",
    "# For all layers: in [batch_size, sent_length, emb_size] / out [batch_size, sent_length, 100]\n",
    "conv1_1 = layers.Conv1D(\n",
    "    100, kernel_size=3, strides=1, padding='same', activation='relu'\n",
    ")(embedding_out)\n",
    "conv1_2 = layers.Conv1D(\n",
    "    100, kernel_size=4, strides=1, padding='same', activation='relu'\n",
    ")(embedding_out)\n",
    "conv1_3 = layers.Conv1D(\n",
    "    100, kernel_size=5, strides=1, padding='same', activation='relu'\n",
    ")(embedding_out)\n",
    "\n",
    "# in previous conve outputs / out [batch_size, sent_length, 300]\n",
    "conv_out = layers.Concatenate(axis=-1)([conv1_1, conv1_2, conv1_3])\n",
    "\n",
    "# Pooling over time operation. This is doing the max pooling over sequence lenth\n",
    "# in other words, each feature map results in a single output\n",
    "# in [batch_size, sent_length, 300] / out [batch_size, 1, 300]\n",
    "pool_over_time_out = layers.MaxPool1D(pool_size=max_seq_length, padding='valid')(conv_out)\n",
    "\n",
    "# Flatten the unit length dimension\n",
    "flatten_out = layers.Flatten()(pool_over_time_out)\n",
    "\n",
    "# Compute the final output\n",
    "out = layers.Dense(\n",
    "    n_classes, activation='softmax',\n",
    "    kernel_regularizer=regularizers.l2(0.001)\n",
    ")(flatten_out)\n",
    "\n",
    "# Define the model\n",
    "cnn_model = Model(inputs=word_id_inputs, outputs=out)\n",
    "\n",
    "# Compile the model with loss/optimzier/metrics\n",
    "cnn_model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Here we train the model with a pre-defined batch size for a number of epochs. We will use a built-in callback.\n",
    "\n",
    "* `ReduceLROnPlateau` - Reduces the learning rate when no improvement detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "39/39 [==============================] - 16s 105ms/step - loss: 1.6184 - accuracy: 0.3777 - val_loss: 1.4437 - val_accuracy: 0.5275 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 1.1090 - accuracy: 0.6665 - val_loss: 0.8600 - val_accuracy: 0.7381 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.6099 - accuracy: 0.8231 - val_loss: 0.5965 - val_accuracy: 0.7949 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.3307 - accuracy: 0.9221 - val_loss: 0.4791 - val_accuracy: 0.8370 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.1809 - accuracy: 0.9682 - val_loss: 0.4367 - val_accuracy: 0.8516 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.1119 - accuracy: 0.9870 - val_loss: 0.4285 - val_accuracy: 0.8663 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0798 - accuracy: 0.9931 - val_loss: 0.4251 - val_accuracy: 0.8700 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0634 - accuracy: 0.9967 - val_loss: 0.4264 - val_accuracy: 0.8736 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0549 - accuracy: 0.9986 - val_loss: 0.4399 - val_accuracy: 0.8718 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9996\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0495 - accuracy: 0.9996 - val_loss: 0.4392 - val_accuracy: 0.8773 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0464 - accuracy: 0.9996 - val_loss: 0.4382 - val_accuracy: 0.8773 - lr: 1.0000e-04\n",
      "Epoch 12/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0460 - accuracy: 0.9996 - val_loss: 0.4388 - val_accuracy: 0.8755 - lr: 1.0000e-04\n",
      "Epoch 13/25\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0457 - accuracy: 0.9996\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0456 - accuracy: 0.9996 - val_loss: 0.4384 - val_accuracy: 0.8755 - lr: 1.0000e-04\n",
      "Epoch 14/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0454 - accuracy: 0.9996 - val_loss: 0.4384 - val_accuracy: 0.8755 - lr: 1.0000e-05\n",
      "Epoch 15/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0453 - accuracy: 0.9996 - val_loss: 0.4385 - val_accuracy: 0.8755 - lr: 1.0000e-05\n",
      "Epoch 16/25\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0453 - accuracy: 0.9996\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0453 - accuracy: 0.9996 - val_loss: 0.4385 - val_accuracy: 0.8755 - lr: 1.0000e-05\n",
      "Epoch 17/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0453 - accuracy: 0.9996 - val_loss: 0.4385 - val_accuracy: 0.8755 - lr: 1.0000e-06\n",
      "Epoch 18/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0453 - accuracy: 0.9996 - val_loss: 0.4385 - val_accuracy: 0.8755 - lr: 1.0000e-06\n",
      "Epoch 19/25\n",
      "38/39 [============================>.] - ETA: 0s - loss: 0.0453 - accuracy: 0.9996\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0452 - accuracy: 0.9996 - val_loss: 0.4385 - val_accuracy: 0.8755 - lr: 1.0000e-06\n",
      "Epoch 20/25\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0452 - accuracy: 0.9996 - val_loss: 0.4385 - val_accuracy: 0.8755 - lr: 1.0000e-06\n",
      "Epoch 21/25\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0452 - accuracy: 0.9996 - val_loss: 0.4385 - val_accuracy: 0.8755 - lr: 1.0000e-06\n",
      "Epoch 22/25\n",
      "39/39 [==============================] - 2s 51ms/step - loss: 0.0452 - accuracy: 0.9996 - val_loss: 0.4385 - val_accuracy: 0.8755 - lr: 1.0000e-06\n",
      "Epoch 23/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0452 - accuracy: 0.9996 - val_loss: 0.4385 - val_accuracy: 0.8755 - lr: 1.0000e-06\n",
      "Epoch 24/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0452 - accuracy: 0.9996 - val_loss: 0.4385 - val_accuracy: 0.8755 - lr: 1.0000e-06\n",
      "Epoch 25/25\n",
      "39/39 [==============================] - 2s 52ms/step - loss: 0.0452 - accuracy: 0.9996 - val_loss: 0.4385 - val_accuracy: 0.8755 - lr: 1.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1528421da88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call backs\n",
    "lr_reduce_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=3, verbose=1,\n",
    "    mode='auto', min_delta=0.0001, min_lr=0.000001\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(\n",
    "    preprocessed_train_sequences, train_labels, \n",
    "    validation_data=(preprocessed_valid_sequences, valid_labels),\n",
    "    batch_size=128, \n",
    "    epochs=25,\n",
    "    callbacks=[lr_reduce_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 36ms/step - loss: 0.3456 - accuracy: 0.8920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.34556981921195984, 'accuracy': 0.8920000195503235}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.evaluate(preprocessed_test_sequences, test_labels, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
