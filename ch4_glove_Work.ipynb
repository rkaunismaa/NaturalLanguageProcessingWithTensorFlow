{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run --gpus all -it -v $(realpath ~/):/tf/All -v /home/rob/Data2:/home/rob/Data2 --env HF_DATASETS_CACHE=/home/rob/Data2/huggingface/datasets --env TRANSFORMERS_CACHE=/home/rob/Data2/huggingface/transformers -p 8888:8888 -p 6006:6006 d139afc9cfb2\n",
    "\n",
    "# This generates 2 files into the glove_embeddings folder ... \n",
    "\n",
    "# 2nd pass, with generate_cooc = False.\n",
    "# Run Date: Thursday, January 19, 2023\n",
    "# Run Time: 00:01:57\n",
    "\n",
    "# First Pass ... \n",
    "# Cell 8 was set to ...\n",
    "# generate_cooc = True\n",
    "# ... which generates the 'cooc_mat.npz' file into the data subfolder ...\n",
    "# Run Date: Thursday, January 19, 2023\n",
    "# Run Time: 00:08:59\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import date\n",
    "\n",
    "startTime = time.time()\n",
    "todaysDate = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only target the 2070 Super ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe: Global Vectors for Word2Vec\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/packt_nlp_tensorflow_2/blob/master/Ch04-Advance-Word-Vectors/ch4_glove.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 16:40:53.808928: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 16:40:54.374878: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-19 16:40:54.374927: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-19 16:40:54.374934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data\n",
    "\n",
    "### Downloading the data\n",
    "\n",
    "This code downloads a [BBC dataset](hhttp://mlg.ucd.ie/files/datasets/bbc-fulltext.zip) consisting of news articles published by BBC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n",
      "bbc-fulltext.zip has already been extracted\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "\n",
    "\n",
    "def download_data(url, data_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    \n",
    "    # Create the data directory if not exist\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
    "    \n",
    "    # If file doesnt exist, download\n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading file...')\n",
    "        filename, _ = urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "  \n",
    "    extract_path = os.path.join(data_dir, 'bbc')\n",
    "    \n",
    "    # If data has not been extracted already, extract data\n",
    "    if not os.path.exists(extract_path):        \n",
    "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "    else:\n",
    "        print(\"bbc-fulltext.zip has already been extracted\")\n",
    "    \n",
    "download_data(url, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data without Preprocessing \n",
    "\n",
    "Here we read all the files and keep them as a list of strings, where each string is a single article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 300.txt\n",
      "Detected 2225 stories\n",
      "865163 words found in the total news set\n",
      "Example words (start):  Gadgets galore on show at fair  The 2005 Consumer \n",
      "Example words (end):   - the Warrior Poet opens in the UK on 21 January.\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_dir):\n",
    "    \n",
    "    # This will contain the full list of stories\n",
    "    news_stories = []\n",
    "    \n",
    "    print(\"Reading files\")\n",
    "    \n",
    "    i = 0 # Just used for printing progress\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        \n",
    "        for fi, f in enumerate(files):\n",
    "            \n",
    "            # We don't read the readme file\n",
    "            if 'README' in f:\n",
    "                continue\n",
    "            \n",
    "            # Printing progress\n",
    "            i += 1\n",
    "            print(\".\"*i, f, end='\\r')\n",
    "            \n",
    "            # Open the file\n",
    "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
    "                \n",
    "                story = []\n",
    "                # Read all the lines\n",
    "                for row in f:\n",
    "                                        \n",
    "                    story.append(row.strip())\n",
    "                    \n",
    "                # Create a single string with all the rows in the doc\n",
    "                story = ' '.join(story)                        \n",
    "                # Add that to the list\n",
    "                news_stories.append(story)  \n",
    "                \n",
    "        print('', end='\\r')\n",
    "        \n",
    "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
    "    return news_stories\n",
    "                \n",
    "  \n",
    "news_stories = read_data(os.path.join('data', 'bbc'))\n",
    "\n",
    "# Printing some stats and sample data\n",
    "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
    "print('Example words (start): ',news_stories[0][:50])\n",
    "print('Example words (end): ',news_stories[-1][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Tokenizer\n",
    "\n",
    "Here we build a tokenizer, that performs simple preprocessing like,\n",
    "\n",
    "* Converting letters to lower case\n",
    "* Removing punctuation\n",
    "\n",
    "and tokenize the strings based on a defined separator. Then each token is converted to an Integer ID, as computers understand numbers, not strings. In the background, the tokenizer builds a word to index dictionary, that defines a unique ID for each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "n_vocab = 15000 + 1\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=n_vocab - 1,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' ', oov_token=''\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the word co-occurrence matrix\n",
    "\n",
    "Why GloVe shine above context window based method is that it employs global statistics of the corpus in to the model (according to authors). This is done by using information from the word co-occurance matrix to optimize the word vectors. Basically, the $X(i,j)$ entry of the co-occurance matrix says how frequent word $i$ to appear near $j$. \n",
    "\n",
    "We also use an optional weighting mechanishm to give more weight to words close together than to the ones further-apart (from experiments section of the paper).\n",
    "\n",
    "**Note**: When generating the matrix for the first time, it will take a significant amount of time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooc matrix of type lil_matrix was loaded from disk\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "def generate_cooc_matrix(text, tokenizer, window_size, n_vocab, use_weighting=True):\n",
    "    \n",
    "    # Convert list of text to list of list of word IDs\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    \n",
    "    # A sparse matrix to retain co-occurrences of words\n",
    "    cooc_mat = lil_matrix((n_vocab, n_vocab), dtype=np.float32)\n",
    "    \n",
    "    # Go through each sequence one by one\n",
    "    for si, sequence in enumerate(sequences):\n",
    "        \n",
    "        # Printing the progress\n",
    "        if (si+1)%100==0:\n",
    "            print('.'*((si+1)//100), f\"{si+1}/{len(sequences)}\", end='\\r')\n",
    "        \n",
    "        # For each target word,\n",
    "        for i, wi in zip(np.arange(window_size, len(sequence)-window_size), sequence[window_size:-window_size]):\n",
    "            \n",
    "            # Get the context window word IDs\n",
    "            context_window = sequence[i-window_size: i+window_size+1]            \n",
    "            \n",
    "            # The weight for the words in the context window (except target word) will be 1\n",
    "            window_weights = np.ones(shape=(window_size*2 + 1,), dtype=np.float32)\n",
    "            window_weights[window_size] = 0.0\n",
    "\n",
    "            if use_weighting:\n",
    "                # If weighting is used, penalize context words based on distance to target word\n",
    "                distances = np.abs(np.arange(-window_size, window_size+1))\n",
    "                distances[window_size] = 1.0\n",
    "                # Update the sparse matrix\n",
    "                cooc_mat[wi, context_window] += window_weights/distances\n",
    "            else:\n",
    "                # Update the sparse matrix\n",
    "                cooc_mat[wi, context_window] += window_weights\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return cooc_mat    \n",
    "\n",
    "# ----------------------------------------- IMPORTANT ---------------------------------------------- #\n",
    "#                                                                                                    #\n",
    "# Set this true or false, depending on whether you want to generate the matrix or reuse the existing #\n",
    "#                                                                                                    #\n",
    "# ---------------------------------------------------------------------------------------------------#\n",
    "generate_cooc = False\n",
    "\n",
    "# Generate the matrix\n",
    "if generate_cooc:\n",
    "    t1 = time.time()\n",
    "    cooc_mat = generate_cooc_matrix(news_stories, tokenizer, 1, n_vocab, True)\n",
    "    t2 = time.time()\n",
    "    print(f\"It took {t2-t1} seconds to generate the co-occurrence matrix\")\n",
    "    \n",
    "    save_npz(os.path.join('data','cooc_mat.npz'), cooc_mat.tocsr())\n",
    "# Load the matrix from disk\n",
    "else:\n",
    "    try:\n",
    "        cooc_mat = load_npz(os.path.join('data','cooc_mat.npz')).tolil()\n",
    "        print(f\"Cooc matrix of type {type(cooc_mat).__name__} was loaded from disk\")\n",
    "    except FileNotFoundError as ex:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find the co-occurrence matrix on the disk. Did you generate the matrix by setting generate_cooc=True?\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the validity of the co-occurrence matrix\n",
    "\n",
    "Here we will see if the context around a given word has sensible words appearing in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f94e94b6850>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e94b6820>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e94b6460>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73c2dc0>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73dd8b0>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73e13a0>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73dd0d0>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73e1e50>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73e9b80>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73ed670>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73edee0>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73e1760>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73f59d0>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73fa4c0>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73faf70>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73fa970>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e73ed910>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e7389280>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e7389d30>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e738f820>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e738fcd0>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e7382b50>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e7395af0>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e739a5e0>,\n",
       "  <matplotlib.axis.XTick at 0x7f94e739ae50>],\n",
       " [Text(0, 0, 'jaime'),\n",
       "  Text(1, 0, 'you'),\n",
       "  Text(2, 0, 'she'),\n",
       "  Text(3, 0, 'understands'),\n",
       "  Text(4, 0, 'it'),\n",
       "  Text(5, 0, 'he'),\n",
       "  Text(6, 0, 'said'),\n",
       "  Text(7, 0, 'cas'),\n",
       "  Text(8, 0, 'i'),\n",
       "  Text(9, 0, 'website'),\n",
       "  Text(10, 0, 'we'),\n",
       "  Text(11, 0, 'school'),\n",
       "  Text(12, 0, 'wales'),\n",
       "  Text(13, 0, \"it's\"),\n",
       "  Text(14, 0, 'on'),\n",
       "  Text(15, 0, 'that'),\n",
       "  Text(16, 0, 'is'),\n",
       "  Text(17, 0, 'a'),\n",
       "  Text(18, 0, ''),\n",
       "  Text(19, 0, 'of'),\n",
       "  Text(20, 0, 'in'),\n",
       "  Text(21, 0, 'and'),\n",
       "  Text(22, 0, 'for'),\n",
       "  Text(23, 0, 'the'),\n",
       "  Text(24, 0, 'bbc')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word = 'sport'\n",
    "assert word in tokenizer.word_index, f\"Word {word} is not in the tokenizer\"\n",
    "assert tokenizer.word_index[word] <= n_vocab, f\"The word {word} is an out of vocabuary word. Please try something else\"\n",
    "\n",
    "# Get the vector of co-occurrences for a given word \n",
    "cooc_vec = np.array(cooc_mat.getrow(tokenizer.word_index[word]).todense()).ravel()\n",
    "# Get indices of words with maximum value\n",
    "max_ind = np.argsort(cooc_vec)[-25:]\n",
    "\n",
    "# Plot the words and values\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.bar(np.arange(0, 25), cooc_vec[max_ind])\n",
    "plt.xticks(ticks=np.arange(0, 25), labels=[tokenizer.index_word[i] for i in max_ind], rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters\n",
    "\n",
    "Here we define several hyperparameters including `batch_size` (amount of samples in a single batch) `embedding_size` (size of embedding vectors) `window_size` (context window size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096 # Data points in a single batch\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "window_size=1 # We use a window size of 1 on either side of target word\n",
    "\n",
    "epochs = 5 # Number of epochs to train for\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid datapoints randomly from a large window without always being deterministic\n",
    "valid_window = 250\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words as well\n",
    "np.random.seed(54321)\n",
    "random.seed(54321)\n",
    "\n",
    "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_term_ids = np.append(\n",
    "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model Computations\n",
    "\n",
    "The model takes two inputs,\n",
    "\n",
    "* A (batch of) context word ID(s) - $i$\n",
    "* A (batch of) target word ID(s) - $j$\n",
    "\n",
    "and computes the following output,\n",
    "\n",
    "$w_i.\\tilde{w}_j + b_i + \\tilde{b}_j$\n",
    "\n",
    "where, $w_i$ is the context embeddings for the words in $i$, $\\tilde{w}_j$ is target embeddings for the words in $j$, $b_i$ and $\\tilde{b}_j$ are two separate biases for context and target spaces. Then the following loss function is used,\n",
    "\n",
    "$J = f(X_{ij}) \\sum_{i,j=1}^{V} (w_i\\tilde{w}_j + b_i + \\tilde{b}_j - log(X_{ij})^2$\n",
    "\n",
    "Here, X_{ij} is the value at (i,j) position in the co-occurrence matrix and f(X_{ij}) is a simple weighting function of X_{ij}. You can see that the loss function looks of the format,\n",
    "\n",
    "$J = A ( B - C ) ^ 2 $\n",
    "\n",
    "Therefore, we will use the mean-squared-error loss and feed in $f(X_{ij})$ values as sample weights during training.\n",
    "\n",
    "---\n",
    "*The behavior of the GloVe word vectors*\n",
    "\n",
    "![The behavior of the GloVe word vectors](notebook_images/04_01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"glove_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " target_embedding (Embedding)   (None, 128)          1920128     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 128)          1920128     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['target_embedding[0][0]',       \n",
      "                                                                  'context_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " target_embedding_bias (Embeddi  (None, 1)           15001       ['input_1[0][0]']                \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " context_embedding_bias (Embedd  (None, 1)           15001       ['input_2[0][0]']                \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 1)            0           ['dot[0][0]',                    \n",
      "                                                                  'target_embedding_bias[0][0]',  \n",
      "                                                                  'context_embedding_bias[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,870,258\n",
      "Trainable params: 3,870,258\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 16:40:55.959930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 16:40:55.963832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 16:40:55.964034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 16:40:55.964365: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 16:40:55.964658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 16:40:55.964815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 16:40:55.964957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 16:40:56.435124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 16:40:56.435325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 16:40:56.435477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 16:40:56.435599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6647 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Define two input layers for context and target words\n",
    "word_i = Input(shape=())\n",
    "word_j = Input(shape=())\n",
    "\n",
    "# Each context and target has their own embeddings (weights and biases)\n",
    "\n",
    "# Embedding weights\n",
    "embeddings_i = Embedding(n_vocab, embedding_size, name='target_embedding')(word_i)\n",
    "embeddings_j = Embedding(n_vocab, embedding_size, name='context_embedding')(word_j)\n",
    "\n",
    "# Embedding biases\n",
    "b_i = Embedding(n_vocab, 1, name='target_embedding_bias')(word_i)    \n",
    "b_j = Embedding(n_vocab, 1, name='context_embedding_bias')(word_j)\n",
    "\n",
    "# Compute the dot product between embedding vectors (i.e. w_i.w_j)\n",
    "ij_dot = Dot(axes=-1)([embeddings_i,embeddings_j])\n",
    "\n",
    "# Add the biases (i.e. w_i.w_j + b_i + b_j )\n",
    "pred = Add()([ij_dot, b_i, b_j])\n",
    "\n",
    "# The final model\n",
    "glove_model = Model(inputs=[word_i, word_j],outputs=pred, name='glove_model')\n",
    "\n",
    "# Glove has a specific loss function with a sound mathematical underpinning\n",
    "# It is a form of mean squared error\n",
    "glove_model.compile(loss=\"mse\", optimizer = 'adam')\n",
    "\n",
    "glove_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data for GloVe model\n",
    "\n",
    "The Glove model we implemented, \n",
    "\n",
    "* Takes two inputs; context words and target words \n",
    "* Computes the mean squared error as, $(\\hat{y}_{ij} - log(X_{ij}))^2$ for the model output $\\hat{y}_{ij}$\n",
    "* Use sample weights returned by $f(X_{ij})$\n",
    "\n",
    "Therefore, in the data generator we return a tuple of,\n",
    "\n",
    "`(inputs, targets, sample weights)`\n",
    "\n",
    "which translates to,\n",
    "\n",
    "`((batch of target words, batch or context words), batch of log(X_{ij}), batch of f(X_{ij})`                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sequences = tokenizer.texts_to_sequences(news_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([9421, 1983, 3399, 6105,  297,  311, 7354, 3301, 3399, 1305]), array([  71, 2152,    6,   30,   12,  351,    7,  219,  478,  198]))\n",
      "[0.6931472 0.6931472 0.6931472 0.        2.7725887 0.        0.6931472\n",
      " 0.        0.        0.       ]\n",
      "[0.03162277 0.03162277 0.03162277 0.         0.24102853 0.\n",
      " 0.03162277 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "def glove_data_generator(\n",
    "    sequences, window_size, batch_size, vocab_size, cooccurrence_matrix, x_max=100.0, alpha=0.75, seed=None\n",
    "):\n",
    "    \"\"\" Generate batches of inputs and targets for GloVe \"\"\"\n",
    "    \n",
    "    # Shuffle the data so that, every epoch, the order of data is different\n",
    "    rand_sequence_ids = np.arange(len(sequences))                    \n",
    "    np.random.shuffle(rand_sequence_ids)\n",
    "\n",
    "    # We will use a sampling table to make sure, we don't oversample stopwords\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "    \n",
    "    # For each story/article\n",
    "    for si in rand_sequence_ids:\n",
    "        \n",
    "        # Generate positive skip-grams while using sub-sampling \n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequences[si], \n",
    "            vocabulary_size=vocab_size, \n",
    "            window_size=window_size, \n",
    "            negative_samples=0.0, \n",
    "            shuffle=False,   \n",
    "            sampling_table=sampling_table,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        # Take targets and context words separately\n",
    "        targets, context = zip(*positive_skip_grams)\n",
    "        targets, context = np.array(targets).ravel(), np.array(context).ravel()\n",
    "        \n",
    "        \n",
    "        x_ij = np.array(cooccurrence_matrix[targets, context].toarray()).ravel()\n",
    "        \n",
    "        # Compute log - Introducing an additive shift to make sure we don't compute log(0)\n",
    "        log_x_ij = np.log(x_ij + 1)\n",
    "        \n",
    "        # Sample weights \n",
    "        # if x < x_max => (x/x_max)**alpha / else => 1        \n",
    "        sample_weights = np.where(x_ij < x_max, (x_ij/x_max)**alpha, 1)\n",
    "        \n",
    "        # If seed is not provided generate a random one\n",
    "        if not seed:\n",
    "            seed = random.randint(0, 10e6)\n",
    "        \n",
    "        # Shuffle data\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(context)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(targets)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(log_x_ij)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(sample_weights)\n",
    "        \n",
    "        # Generate a batch or data in the format \n",
    "        # ((target words, context words), log(X_ij) <- true targets, f(X_ij) <- sample weights)\n",
    "        for eg_id_start in range(0, context.shape[0], batch_size):            \n",
    "            yield (\n",
    "                targets[eg_id_start: min(eg_id_start+batch_size, targets.shape[0])], \n",
    "                context[eg_id_start: min(eg_id_start+batch_size, context.shape[0])]\n",
    "            ), log_x_ij[eg_id_start: min(eg_id_start+batch_size, x_ij.shape[0])], \\\n",
    "            sample_weights[eg_id_start: min(eg_id_start+batch_size, sample_weights.shape[0])]\n",
    "\n",
    "\n",
    "# Generate some data\n",
    "news_glove_data_gen = glove_data_generator(\n",
    "    news_sequences, 2, 10, n_vocab, cooc_mat\n",
    ")\n",
    "\n",
    "for x, y, z in news_glove_data_gen:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(z)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Here we train the GloVe model we defined above. We train for `epochs` and at the end of each epoch, we compute word similarities on a set of chosen validation words (`valid_term_ids`). Similar to in Chapter 3, we use a Keras callback to compute the most similar words.\n",
    "\n",
    "### Calculating Word Similarities\n",
    "\n",
    "We calculate the similarity between two given words in terms of the cosine distance. To do this efficiently we use matrix operations to do so, as shown below. Furthermore, we define the computations as a callback, which will automatically run at the end of an epoch during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
    "        \n",
    "        self.valid_term_ids = valid_term_ids\n",
    "        self.model_with_embeddings = model_with_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\" Validation logic \"\"\"\n",
    "                \n",
    "        # We will use context embeddings to get the most similar words\n",
    "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
    "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
    "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
    "        \n",
    "        # Get the embeddings corresponding to valid_term_ids\n",
    "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
    "        \n",
    "        # Compute the similarity between valid_term_ids and all the embeddings\n",
    "        # V x d (d x D) => V x D\n",
    "        top_k = 5 # Top k items will be displayed\n",
    "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
    "        \n",
    "        # Invert similarity matrix to negative\n",
    "        # Ignore the first one because that would be the same word as the probe word\n",
    "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
    "                \n",
    "        # Print the output\n",
    "        for i, term_id in enumerate(valid_term_ids):\n",
    "            \n",
    "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j > 1])\n",
    "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
    "        \n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 16:40:57.969597: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1ea3d650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-19 16:40:57.969632: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2070 SUPER, Compute Capability 7.5\n",
      "2023-01-19 16:40:57.973269: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-01-19 16:40:58.087128: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2215/Unknown - 69s 31ms/step - loss: 0.5989election: attorney, motors, director, manager, network\n",
      "me: him, comes, us, win, japan\n",
      "with: together, reached, shared, very, straight\n",
      "you: we, they, also, still, now\n",
      "were: but, are, was, is, if\n",
      "win: us, time, parliament, comes, cases\n",
      "those: which, but, plans, won, performed\n",
      "music: cameras, came, choice, figures, one\n",
      "also: made, they, now, used, but\n",
      "international: my, or, took, irish, us\n",
      "best: category, supporting, original, growing, bafta\n",
      "him: another, comes, role, game, any\n",
      "too: so, how, very, some, fair\n",
      "into: current, straight, over, from, good\n",
      "through: into, over, current, straight, from\n",
      "mr: tony, gordon, resignation, jack, cherie\n",
      "stay: win, value, parliament, work, which\n",
      "kept: playing, won, urban, sixth, japan\n",
      "agreement: victory, information, comes, him, ways\n",
      "australia: 2005, comes, fans, us\n",
      "unit: took, one, service, all, thompson\n",
      "serious: game, broadband, movie, pre, or\n",
      "field: game, issue, my, independent\n",
      "debate: time, thursday, win, driven, place\n",
      "individual: making, won, company, start, current\n",
      "send: very, just, fans, one, strong\n",
      "argued: said, when, added, says, thought\n",
      "camera: mobile, lines, operators, pager, replacement\n",
      "machine: very, after, done, him, put\n",
      "favourite: programs, hidden, works, company, get\n",
      "trust: lost, within, straight, club, current\n",
      "significant: game, or, 10, goes, us\n",
      "\n",
      "\n",
      "2225/2225 [==============================] - 70s 31ms/step - loss: 0.5963\n",
      "Epoch: 2/5 started\n",
      "   2223/Unknown - 13s 6ms/step - loss: 0.0367election: attorney, forthcoming, motors, upcoming, november's\n",
      "me: work, comes, win, concerned, development\n",
      "with: together, bbc's, honoured, shared, gizmondo\n",
      "you: we, they, still, know, done\n",
      "were: are, but, need, doubt, when\n",
      "win: hold, least, rise, order, comes\n",
      "those: showed, plans, need, but, considered\n",
      "music: cameras, lifestyle, revolution, subscriber, legitimate\n",
      "also: give, play, then, allow, now\n",
      "international: increase, fund, entertainment, november, football\n",
      "best: supporting, category, actress, original, actor\n",
      "him: role, look, least, talks, comes\n",
      "too: so, bigger, how, larger, strain\n",
      "into: through, current, task, played, speeds\n",
      "through: current, over, benefits, government's, release\n",
      "mr: bernie, malcolm, tony, jack, resignation\n",
      "stay: win, get, keen, designed, hold\n",
      "kept: urban, japan, me, released, here\n",
      "agreement: him, look, role, mps, comes\n",
      "australia: 2005, least, wales, cardiff, based\n",
      "unit: took, one, avoid, arm, conflicting\n",
      "serious: lot, decision, download, arrest, men\n",
      "field: issue, decision, hopes, game, independent\n",
      "debate: least, thursday, developed, biggest, win\n",
      "individual: start, decision, price, sell, lot\n",
      "send: failed, lot, reached, start, cut\n",
      "argued: said, says, added, believes, when\n",
      "camera: mobile, symbian, operators, replacement, shirts\n",
      "machine: start, opening, run, use, defend\n",
      "favourite: basis, surprise, bluetooth, rebound, danger\n",
      "trust: club, within, trading, decision, lost\n",
      "significant: stake, pirated, game, issue, price\n",
      "\n",
      "\n",
      "2225/2225 [==============================] - 13s 6ms/step - loss: 0.0367\n",
      "Epoch: 3/5 started\n",
      "   2214/Unknown - 11s 5ms/step - loss: 0.0159election: attorney, forthcoming, larsson, motors, campbell's\n",
      "me: compared, look, thursday, aimed, looked\n",
      "with: honoured, states', bbc's, together, priorities\n",
      "you: we, they, afford, goodness, god\n",
      "were: are, squeezed, when, need, because\n",
      "win: hold, according, order, rise, aimed\n",
      "those: died, blume, means, haves, need\n",
      "music: cameras, subscriber, revolution, lifestyle, terrestrial\n",
      "also: play, given, give, allow, then\n",
      "international: board, glasgow, edinburgh, fund, increase\n",
      "best: supporting, category, counterparts, actress, original\n",
      "him: me, fightstar, talks, them, look\n",
      "too: larger, trailed, how, bigger, pretty\n",
      "into: hutton, through, current, offshore, introduction\n",
      "through: speeds, government's, door, ahead, 'may\n",
      "mr: bernie, 63, article, tony, malcolm\n",
      "stay: keen, designed, play, create, win\n",
      "kept: grabbed, information, me, forget, japan\n",
      "agreement: dogs, shrugged, stuck, look, references\n",
      "australia: order, cardiff, 2005, based, aimed\n",
      "unit: niche, castrogiovanni, avoid, all, disadvantaged\n",
      "serious: lot, series, men, similar, decision\n",
      "field: average, easy, alternative, effort, flat\n",
      "debate: least, aimed, thursday, driven, independence\n",
      "individual: height, sell, start, beginning, price\n",
      "send: create, move, due, lot, start\n",
      "argued: says, said, means, added, claimed\n",
      "camera: symbian, mobile, mobile's, lines, shirts\n",
      "machine: holder, label, types, course, notebook\n",
      "favourite: earthquake, retain, facing, basis, dire\n",
      "trust: pent, within, club, introduction, imf\n",
      "significant: lot, stake, pirated, statement, sort\n",
      "\n",
      "\n",
      "2225/2225 [==============================] - 11s 5ms/step - loss: 0.0159\n",
      "Epoch: 4/5 started\n",
      "   2200/Unknown - 10s 5ms/step - loss: 0.0116election: attorney, forthcoming, conservatives', larsson, guides\n",
      "me: compared, interact, responsible, cope, incorporated\n",
      "with: honoured, bbc's, gizmondo, states', dates\n",
      "you: we, afford, god, they, goodness\n",
      "were: are, lured, because, squeezed, relieved\n",
      "win: aimed, wait, least, 2007, sophos\n",
      "those: died, murderers, blume, won, haves\n",
      "music: cameras, divide, subscriber, legitimate, revolution\n",
      "also: given, give, then, play, remain\n",
      "international: board, edinburgh, glasgow, fund, international's\n",
      "best: supporting, hijack, actress, bests, original\n",
      "him: me, cope, along, millions, compared\n",
      "too: trailed, larger, bigger, interference, wow\n",
      "into: hutton, through, offshore, deposit, door\n",
      "through: ahead, door, course, benefits, introduction\n",
      "mr: bernie, 63, wes, gordon, tony\n",
      "stay: likely, win, keen, grow, freedom\n",
      "kept: grabbed, information, founded, steal, acknowledge\n",
      "agreement: shrugged, dogs, stuck, deal, letters\n",
      "australia: terms, favour, order, aimed, 1999\n",
      "unit: values, yukos's, castrogiovanni, crossover, niche\n",
      "serious: similar, lot, months', series, men\n",
      "field: average, estimated, willingness, honorary, easy\n",
      "debate: table, least, aimed, independence, logs\n",
      "individual: start, extra, addiction, beginning, crucible\n",
      "send: build, create, start, lot, thing\n",
      "argued: says, means, said, wanted, gives\n",
      "camera: symbian, mobile, mobile's, pager, shirts\n",
      "machine: holder, sundance, context, label, closely\n",
      "favourite: contributor, rigid, bit, a, billboard's\n",
      "trust: introduction, club, within, uk's, chartered\n",
      "significant: couple, lot, pirated, statement, around\n",
      "\n",
      "\n",
      "2225/2225 [==============================] - 10s 5ms/step - loss: 0.0116\n",
      "Epoch: 5/5 started\n",
      "   2215/Unknown - 9s 4ms/step - loss: 0.0094election: attorney, conservatives', november's, guides, campbell's\n",
      "me: responsible, interact, cope, compared, incorporated\n",
      "with: gizmondo, bbc's, states', passport, breach\n",
      "you: we, afford, god, they, goodness\n",
      "were: aren't, originated, lured, are, mori\n",
      "win: order, aimed, interested, sophos, least\n",
      "those: died, blume, murderers, obe, paniccia\n",
      "music: divide, cameras, refuseniks, subscriber, mp3\n",
      "also: given, hoping, received, made, give\n",
      "international: glasgow, board, edinburgh, international's, fund\n",
      "best: supporting, counterparts, bests, original, hijack\n",
      "him: along, cope, compared, interact, incorporated\n",
      "too: bigger, larger, pretty, trailed, stronger\n",
      "into: hutton, through, door, current, fatal\n",
      "through: door, ahead, shut, crack, into\n",
      "mr: 63, bernie, wes, frederick, tony\n",
      "stay: win, freedom, surreptitious, npd, icm\n",
      "kept: grabbed, information, sources, steal, founded\n",
      "agreement: ended, bundle, dogs, fought, shrugged\n",
      "australia: interested, faith, terms, favour, order\n",
      "unit: values, castrogiovanni, auctioned, niche, repayments\n",
      "serious: decade, payers, conflicting, cyprus, series\n",
      "field: average, resume, easy, issue, increase\n",
      "debate: developed, table, shrugged, aimed, sophos\n",
      "individual: extra, series, start, lot, awful\n",
      "send: build, lot, create, pornographic, move\n",
      "argued: says, said, added, context, wanted\n",
      "camera: mobile, symbian, mobile's, cripple, pager\n",
      "machine: nitoglia, deacon, cullen, holder, clips\n",
      "favourite: invasive, bit, 60th, beta, magazine's\n",
      "trust: pent, introduction, issue, club, rest\n",
      "significant: surgery, commissioning, five's, stake, forgotten\n",
      "\n",
      "\n",
      "2225/2225 [==============================] - 9s 4ms/step - loss: 0.0094\n"
     ]
    }
   ],
   "source": [
    "glove_validation_callback = ValidationCallback(valid_term_ids, glove_model, tokenizer)\n",
    "\n",
    "# Train the model for several epochs\n",
    "for ei in range(epochs):\n",
    "    \n",
    "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
    "    \n",
    "    news_glove_data_gen = glove_data_generator(\n",
    "        news_sequences, window_size, batch_size, n_vocab, cooc_mat\n",
    "    )\n",
    "    \n",
    "    glove_model.fit(\n",
    "        news_glove_data_gen, epochs=1, \n",
    "        callbacks=glove_validation_callback,        \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the embeddings\n",
    "We save the learned embeddings to the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(model, tokenizer, vocab_size, save_dir):\n",
    "    \n",
    "    # Create the directory if doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Get the words sorted according to their ID from the tokenizer\n",
    "    _, words_sorted = zip(*sorted(list(tokenizer.index_word.items()), key=lambda x: x[0])[:vocab_size-1])\n",
    "    # Add one word in front to represent the reserved ID (0)\n",
    "    words_sorted = [None] + list(words_sorted)\n",
    "    \n",
    "    # Create a new array by concatenating embeddings and bias\n",
    "    \n",
    "    context_embedding_weights = model.get_layer(\"context_embedding\").get_weights()[0]\n",
    "    context_embedding_bias = model.get_layer(\"context_embedding_bias\").get_weights()[0]\n",
    "    context_embedding = np.concatenate([context_embedding_weights, context_embedding_bias], axis=1)\n",
    "    \n",
    "    target_embedding_weights = model.get_layer(\"target_embedding\").get_weights()[0]\n",
    "    target_embedding_bias = model.get_layer(\"target_embedding_bias\").get_weights()[0]\n",
    "    target_embedding = np.concatenate([target_embedding_weights, target_embedding_bias], axis=1)\n",
    "    \n",
    "    # Save the array as a Pandas DataFrames\n",
    "    pd.DataFrame(\n",
    "        context_embedding, \n",
    "        index = words_sorted\n",
    "    ).to_pickle(os.path.join(save_dir, \"context_embedding_and_bias.pkl\"))\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        target_embedding, \n",
    "        index = words_sorted\n",
    "    ).to_pickle(os.path.join(save_dir, \"target_embedding_and_bias.pkl\"))\n",
    "\n",
    "    \n",
    "save_embeddings(glove_model, tokenizer, n_vocab, save_dir='glove_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Run Date: Thursday, January 19, 2023\n",
      "# Run Time: 00:01:57\n"
     ]
    }
   ],
   "source": [
    "endTime = time.time()\n",
    "elapsedTime = time.strftime(\"%H:%M:%S\", time.gmtime(endTime - startTime))\n",
    "\n",
    "print(todaysDate.strftime('# Run Date: %A, %B %d, %Y'))\n",
    "print(f\"# Run Time: {elapsedTime}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
