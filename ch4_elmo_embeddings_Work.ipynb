{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38a1a9a-c072-466e-b535-eda0d8948c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run --gpus all -it -v $(realpath ~/):/tf/All -v /home/rob/Data2:/home/rob/Data2 --env HF_DATASETS_CACHE=/home/rob/Data2/huggingface/datasets --env TRANSFORMERS_CACHE=/home/rob/Data2/huggingface/transformers -p 8888:8888 -p 6006:6006 d139afc9cfb2\n",
    "\n",
    "# This generates the 'elmo_embeddings.pkl' file into \n",
    "# the elmo_embeddings folder.\n",
    "\n",
    "# 2nd Pass ... \n",
    "# Run Date: Thursday, January 19, 2023\n",
    "# Run Time: 00:07:17\n",
    "\n",
    "# First pass ... \n",
    "# Run Date: Thursday, January 19, 2023\n",
    "# Run Time: 00:12:18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e030b0-1f28-4aa7-af71-bdcbeb8f5eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import date\n",
    "\n",
    "startTime = time.time()\n",
    "todaysDate = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4857b2a6-0bac-4754-83f7-478ad9307cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only target the 2070 Super ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e0e19",
   "metadata": {},
   "source": [
    "## Using ELMo Embeddings\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/packt_nlp_tensorflow_2/blob/master/Ch04-Advance-Word-Vectors/ch4_elmo_embeddings.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6647d3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 17:02:00.370093: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 17:02:00.961430: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-19 17:02:00.961477: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-19 17:02:00.961484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n",
      "env: TFHUB_CACHE_DIR=./tfhub_modules\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "# Making sure we cache the models and are not downloaded all the time\n",
    "%env TFHUB_CACHE_DIR=./tfhub_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08272665",
   "metadata": {},
   "source": [
    "## Using pre-trained ELMo Model\n",
    "\n",
    "### Downloading the ELMo Model from TFHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c5a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 17:02:01.927944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:02:01.931836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:02:01.932434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:02:01.933182: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 17:02:01.933641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:02:01.939076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:02:01.939250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:02:02.384730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:02:02.384913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:02:02.385061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 17:02:02.385239: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-01-19 17:02:02.385267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6647 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Remove any ongoing sessions\n",
    "K.clear_session()\n",
    "\n",
    "# Download the ELMo model and save to disk\n",
    "elmo_layer = hub.KerasLayer(\"https://tfhub.dev/google/elmo/3\", signature=\"tokens\",signature_outputs_as_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e46bcf",
   "metadata": {},
   "source": [
    "### Formatting the input for ELMo\n",
    "\n",
    "ELMo expects the inputs to be in a specific format. Here we write a function to get the input in that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0f0cdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': <tf.Tensor: shape=(2, 6), dtype=string, numpy=\n",
      "array([[b'the', b'cat', b'sat', b'on', b'the', b'mat'],\n",
      "       [b'the', b'mat', b'sat', b'', b'', b'']], dtype=object)>, 'sequence_len': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([6, 3], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "def format_text_for_elmo(texts, lower=True, split=\" \", max_len=None):\n",
    "    \n",
    "    \"\"\" Formats a given text for the ELMo model (takes in a list of strings) \"\"\"\n",
    "        \n",
    "    token_inputs = [] # Maintains individual tokens\n",
    "    token_lengths = [] # Maintains the length of each sequence\n",
    "    \n",
    "    max_len_inferred = 0 # We keep a variable to matain the max length of the input\n",
    "    \n",
    "    # Go through each text (string)\n",
    "    for text in texts:    \n",
    "        \n",
    "        # Process the text and get a list of tokens\n",
    "        tokens = tf.keras.preprocessing.text.text_to_word_sequence(text, lower=lower, split=split)\n",
    "        \n",
    "        # Add the tokens \n",
    "        token_inputs.append(tokens)                   \n",
    "        \n",
    "        # Compute the max length for the collection of sequences\n",
    "        if len(tokens)>max_len_inferred:\n",
    "            max_len_inferred = len(tokens)\n",
    "    \n",
    "    # It's important to make sure the maximum token length is only as large as the longest input in the sequence\n",
    "    # You can't have arbitrarily large length as the maximum length. Otherwise, you'll get this error.\n",
    "    #InvalidArgumentError:  Incompatible shapes: [2,6,1] vs. [2,10,1024]\n",
    "    #    [[node mul (defined at .../python3.6/site-packages/tensorflow_hub/module_v2.py:106) ]] [Op:__inference_pruned_3391]\n",
    "    \n",
    "    # Here we make sure max_len is only as large as the longest input\n",
    "    if max_len and max_len_inferred < max_len:\n",
    "        max_len = max_len_inferred\n",
    "    if not max_len:\n",
    "        max_len = max_len_inferred\n",
    "    \n",
    "    # Go through each token sequence and modify sequences to have same length\n",
    "    for i, token_seq in enumerate(token_inputs):\n",
    "        \n",
    "        token_lengths.append(min(len(token_seq), max_len))\n",
    "        \n",
    "        # If the maximum length is less than input length, truncate\n",
    "        if max_len < len(token_seq):\n",
    "            token_seq = token_seq[:max_len]            \n",
    "        # If the maximum length is greater than or equal to input length, add padding as needed\n",
    "        else:            \n",
    "            token_seq = token_seq+[\"\"]*(max_len-len(token_seq))\n",
    "                \n",
    "        assert len(token_seq)==max_len\n",
    "        \n",
    "        token_inputs[i] = token_seq\n",
    "    \n",
    "    # Return the final output\n",
    "    return {\n",
    "        \"tokens\": tf.constant(token_inputs), \n",
    "        \"sequence_len\": tf.constant(token_lengths)\n",
    "    }\n",
    "\n",
    "\n",
    "print(format_text_for_elmo([\"the cat sat on the mat\", \"the mat sat\"], max_len=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45986d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 17:02:04.956188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor under key=lstm_outputs1 is a (5, 6, 1024) shaped Tensor\n",
      "Tensor under key=elmo is a (5, 6, 1024) shaped Tensor\n",
      "Tensor under key=lstm_outputs2 is a (5, 6, 1024) shaped Tensor\n",
      "Tensor under key=sequence_len is a (5,) shaped Tensor\n",
      "Tensor under key=word_emb is a (5, 6, 512) shaped Tensor\n",
      "Tensor under key=default is a (5, 1024) shaped Tensor\n"
     ]
    }
   ],
   "source": [
    "# Titles of 001.txt - 005.txt in bbc/business\n",
    "elmo_inputs = format_text_for_elmo([\n",
    "    \"Ad sales boost Time Warner profit\",\n",
    "    \"Dollar gains on Greenspan speech\",\n",
    "    \"Yukos unit buyer faces loan claim\",\n",
    "    \"High fuel prices hit BA's profits\",\n",
    "    \"Pernod takeover talk lifts Domecq\"\n",
    "])\n",
    "\n",
    "# Get the result from ELMo\n",
    "elmo_result = elmo_layer(elmo_inputs)\n",
    "\n",
    "# Print the result\n",
    "for k,v in elmo_result.items():    \n",
    "    print(f\"Tensor under key={k} is a {v.shape} shaped Tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb329b",
   "metadata": {},
   "source": [
    "## Generating Document Embeddings with ELMo\n",
    "\n",
    "### Downloading the data\n",
    "\n",
    "This code downloads a [BBC dataset](hhttp://mlg.ucd.ie/files/datasets/bbc-fulltext.zip) consisting of news articles published by BBC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801a2bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n",
      "bbc-fulltext.zip has already been extracted\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "\n",
    "\n",
    "def download_data(url, data_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    \n",
    "    # Create the data directory if not exist\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
    "    \n",
    "    # If file doesnt exist, download\n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading file...')\n",
    "        filename, _ = urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "  \n",
    "    extract_path = os.path.join(data_dir, 'bbc')\n",
    "    \n",
    "    # If data has not been extracted already, extract data\n",
    "    if not os.path.exists(extract_path):        \n",
    "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "    else:\n",
    "        print(\"bbc-fulltext.zip has already been extracted\")\n",
    "    \n",
    "download_data(url, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416dba6e",
   "metadata": {},
   "source": [
    "### Read Data without Preprocessing \n",
    "\n",
    "Here we read all the files and keep them as a list of strings, where each string is a single article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b0a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 300.txt\n",
      "Detected 2225 stories\n",
      "865163 words found in the total news set\n",
      "Example words (start):  Gadgets galore on show at fair  The 2005 Consumer \n",
      "Example words (end):   - the Warrior Poet opens in the UK on 21 January.\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_dir):\n",
    "    \n",
    "    # This will contain the full list of stories\n",
    "    news_stories = []    \n",
    "    filenames = []\n",
    "    print(\"Reading files\")\n",
    "    \n",
    "    i = 0 # Just used for printing progress\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        \n",
    "        for fi, f in enumerate(files):\n",
    "            \n",
    "            # We don't read the readme file\n",
    "            if 'README' in f:\n",
    "                continue\n",
    "            \n",
    "            # Printing progress\n",
    "            i += 1\n",
    "            print(\".\"*i, f, end='\\r')\n",
    "            \n",
    "            # Open the file\n",
    "            with open(os.path.join(root, f), encoding='latin-1') as text_file:\n",
    "                \n",
    "                story = []\n",
    "                # Read all the lines\n",
    "                for row in text_file:\n",
    "                                        \n",
    "                    story.append(row.strip())\n",
    "                    \n",
    "                # Create a single string with all the rows in the doc\n",
    "                story = ' '.join(story)                        \n",
    "                # Add that to the list\n",
    "                news_stories.append(story)  \n",
    "                filenames.append(os.path.join(root, f))\n",
    "                \n",
    "        print('', end='\\r')\n",
    "        \n",
    "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
    "    return news_stories, filenames\n",
    "                \n",
    "  \n",
    "news_stories, filenames = read_data(os.path.join('data', 'bbc'))\n",
    "\n",
    "# Printing some stats and sample data\n",
    "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
    "print('Example words (start): ',news_stories[0][:50])\n",
    "print('Example words (end): ',news_stories[-1][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ff4ec",
   "metadata": {},
   "source": [
    "### Check the length statistics \n",
    "\n",
    "Here we look at the 95-percientile in order to decide a good sequence length for inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3337475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2225.000000\n",
       "mean      388.837303\n",
       "std       241.484273\n",
       "min        91.000000\n",
       "5%        164.200000\n",
       "50%       336.000000\n",
       "95%       736.800000\n",
       "max      4489.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.Series([len(x.split(' ')) for x in news_stories]).describe(percentiles=[0.05, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a8307",
   "metadata": {},
   "source": [
    "### Compute the document embeddings\n",
    "\n",
    "ELMo provides several outputs as the output (in the form of a dictionary). The most important output is in a key called `default` which is the averaged vector resulting from vectors produced for all the tokens in the input. We will use this as the document embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d915657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 17:04:12.581481: W tensorflow/tsl/framework/bfc_allocator.cc:360] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "news_elmo_embeddings = []\n",
    "\n",
    "# Go through batches\n",
    "for i in range(0, len(news_stories), batch_size):\n",
    "    \n",
    "    # Print progress\n",
    "    print('.', end='')\n",
    "    # Format ELMo inputs\n",
    "    elmo_inputs = format_text_for_elmo(news_stories[i: min(i+batch_size, len(news_stories))], max_len=768)    \n",
    "    # Get the result stored in default\n",
    "    elmo_result = elmo_layer(elmo_inputs)[\"default\"]\n",
    "    # Add that to a list\n",
    "    news_elmo_embeddings.append(elmo_result)\n",
    "\n",
    "# Create an array\n",
    "news_elmo_embeddings = np.concatenate(news_elmo_embeddings, axis=0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb919f",
   "metadata": {},
   "source": [
    "### Save the embeddings to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaf38bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to disk\n",
    "os.makedirs('elmo_embeddings', exist_ok=True)\n",
    "\n",
    "pd.DataFrame(\n",
    "    news_elmo_embeddings, index=filenames\n",
    ").to_pickle(\n",
    "    os.path.join('elmo_embeddings', 'elmo_embeddings.pkl')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac4aaa9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data/bbc/tech/174.txt</th>\n",
       "      <td>0.012744</td>\n",
       "      <td>0.023062</td>\n",
       "      <td>-0.035946</td>\n",
       "      <td>-0.090556</td>\n",
       "      <td>0.144047</td>\n",
       "      <td>0.166769</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>0.342860</td>\n",
       "      <td>-0.161670</td>\n",
       "      <td>-0.082526</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340656</td>\n",
       "      <td>0.244855</td>\n",
       "      <td>0.030709</td>\n",
       "      <td>0.253689</td>\n",
       "      <td>0.078406</td>\n",
       "      <td>0.044581</td>\n",
       "      <td>0.372656</td>\n",
       "      <td>-0.006501</td>\n",
       "      <td>0.273694</td>\n",
       "      <td>0.057726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/tech/170.txt</th>\n",
       "      <td>0.256800</td>\n",
       "      <td>-0.076643</td>\n",
       "      <td>-0.005060</td>\n",
       "      <td>-0.063172</td>\n",
       "      <td>0.138440</td>\n",
       "      <td>0.189914</td>\n",
       "      <td>0.182261</td>\n",
       "      <td>0.326142</td>\n",
       "      <td>-0.117797</td>\n",
       "      <td>0.026969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171903</td>\n",
       "      <td>0.165076</td>\n",
       "      <td>-0.113165</td>\n",
       "      <td>0.327153</td>\n",
       "      <td>0.209255</td>\n",
       "      <td>0.082908</td>\n",
       "      <td>0.258461</td>\n",
       "      <td>-0.122383</td>\n",
       "      <td>0.307110</td>\n",
       "      <td>-0.029122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/tech/302.txt</th>\n",
       "      <td>0.178142</td>\n",
       "      <td>0.275478</td>\n",
       "      <td>-0.044081</td>\n",
       "      <td>0.069659</td>\n",
       "      <td>0.219834</td>\n",
       "      <td>0.160469</td>\n",
       "      <td>-0.008855</td>\n",
       "      <td>0.358277</td>\n",
       "      <td>0.126847</td>\n",
       "      <td>-0.048421</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.290554</td>\n",
       "      <td>0.097650</td>\n",
       "      <td>-0.033120</td>\n",
       "      <td>0.317354</td>\n",
       "      <td>-0.011647</td>\n",
       "      <td>0.147325</td>\n",
       "      <td>0.441159</td>\n",
       "      <td>-0.065567</td>\n",
       "      <td>0.119717</td>\n",
       "      <td>-0.027908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/tech/256.txt</th>\n",
       "      <td>0.113356</td>\n",
       "      <td>-0.150963</td>\n",
       "      <td>-0.054547</td>\n",
       "      <td>0.078308</td>\n",
       "      <td>0.010922</td>\n",
       "      <td>0.414599</td>\n",
       "      <td>-0.085480</td>\n",
       "      <td>0.392278</td>\n",
       "      <td>0.147020</td>\n",
       "      <td>-0.126910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291242</td>\n",
       "      <td>0.131670</td>\n",
       "      <td>0.126649</td>\n",
       "      <td>0.161829</td>\n",
       "      <td>0.085308</td>\n",
       "      <td>0.041790</td>\n",
       "      <td>0.132907</td>\n",
       "      <td>-0.010835</td>\n",
       "      <td>0.489633</td>\n",
       "      <td>-0.170547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/tech/211.txt</th>\n",
       "      <td>0.161718</td>\n",
       "      <td>0.072896</td>\n",
       "      <td>-0.033089</td>\n",
       "      <td>-0.112782</td>\n",
       "      <td>0.426878</td>\n",
       "      <td>0.299916</td>\n",
       "      <td>0.084742</td>\n",
       "      <td>0.284771</td>\n",
       "      <td>-0.164555</td>\n",
       "      <td>0.051139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091179</td>\n",
       "      <td>0.205737</td>\n",
       "      <td>-0.067422</td>\n",
       "      <td>0.088594</td>\n",
       "      <td>0.118239</td>\n",
       "      <td>0.079229</td>\n",
       "      <td>0.402150</td>\n",
       "      <td>-0.011402</td>\n",
       "      <td>0.355481</td>\n",
       "      <td>-0.067534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/entertainment/015.txt</th>\n",
       "      <td>-0.188401</td>\n",
       "      <td>-0.300901</td>\n",
       "      <td>0.023362</td>\n",
       "      <td>-0.128732</td>\n",
       "      <td>0.306804</td>\n",
       "      <td>0.241635</td>\n",
       "      <td>0.068264</td>\n",
       "      <td>0.128954</td>\n",
       "      <td>-0.128111</td>\n",
       "      <td>-0.200275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253069</td>\n",
       "      <td>0.376614</td>\n",
       "      <td>-0.215489</td>\n",
       "      <td>0.362154</td>\n",
       "      <td>-0.009835</td>\n",
       "      <td>-0.058118</td>\n",
       "      <td>0.197030</td>\n",
       "      <td>0.056924</td>\n",
       "      <td>0.493857</td>\n",
       "      <td>-0.102432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/entertainment/345.txt</th>\n",
       "      <td>-0.178258</td>\n",
       "      <td>-0.212788</td>\n",
       "      <td>0.019289</td>\n",
       "      <td>-0.097152</td>\n",
       "      <td>0.224833</td>\n",
       "      <td>0.122836</td>\n",
       "      <td>-0.037274</td>\n",
       "      <td>-0.269267</td>\n",
       "      <td>-0.330137</td>\n",
       "      <td>-0.047496</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123795</td>\n",
       "      <td>0.075508</td>\n",
       "      <td>-0.240921</td>\n",
       "      <td>0.167021</td>\n",
       "      <td>0.135244</td>\n",
       "      <td>0.192209</td>\n",
       "      <td>0.115502</td>\n",
       "      <td>-0.237828</td>\n",
       "      <td>0.562652</td>\n",
       "      <td>-0.099549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/entertainment/024.txt</th>\n",
       "      <td>-0.376309</td>\n",
       "      <td>-0.317978</td>\n",
       "      <td>0.054523</td>\n",
       "      <td>-0.213948</td>\n",
       "      <td>0.229501</td>\n",
       "      <td>0.119958</td>\n",
       "      <td>0.037880</td>\n",
       "      <td>-0.051839</td>\n",
       "      <td>-0.106681</td>\n",
       "      <td>-0.058162</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211875</td>\n",
       "      <td>0.328167</td>\n",
       "      <td>-0.178842</td>\n",
       "      <td>0.107765</td>\n",
       "      <td>-0.138602</td>\n",
       "      <td>-0.028439</td>\n",
       "      <td>0.088771</td>\n",
       "      <td>-0.069143</td>\n",
       "      <td>0.444389</td>\n",
       "      <td>-0.067932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/entertainment/299.txt</th>\n",
       "      <td>0.129563</td>\n",
       "      <td>-0.149498</td>\n",
       "      <td>-0.086580</td>\n",
       "      <td>-0.084570</td>\n",
       "      <td>0.133638</td>\n",
       "      <td>0.179378</td>\n",
       "      <td>0.126872</td>\n",
       "      <td>0.171507</td>\n",
       "      <td>0.190518</td>\n",
       "      <td>-0.126271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183596</td>\n",
       "      <td>0.280337</td>\n",
       "      <td>-0.227419</td>\n",
       "      <td>0.318273</td>\n",
       "      <td>0.012311</td>\n",
       "      <td>0.118652</td>\n",
       "      <td>0.127632</td>\n",
       "      <td>-0.050924</td>\n",
       "      <td>0.435821</td>\n",
       "      <td>-0.108651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/bbc/entertainment/300.txt</th>\n",
       "      <td>-0.188012</td>\n",
       "      <td>-0.146533</td>\n",
       "      <td>-0.042753</td>\n",
       "      <td>-0.151030</td>\n",
       "      <td>0.167452</td>\n",
       "      <td>0.264067</td>\n",
       "      <td>-0.069800</td>\n",
       "      <td>0.476115</td>\n",
       "      <td>0.135215</td>\n",
       "      <td>-0.078510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133540</td>\n",
       "      <td>0.364964</td>\n",
       "      <td>-0.131337</td>\n",
       "      <td>0.285388</td>\n",
       "      <td>0.203801</td>\n",
       "      <td>0.129833</td>\n",
       "      <td>-0.031789</td>\n",
       "      <td>-0.225053</td>\n",
       "      <td>0.574074</td>\n",
       "      <td>-0.091809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0         1         2         3     \\\n",
       "data/bbc/tech/174.txt           0.012744  0.023062 -0.035946 -0.090556   \n",
       "data/bbc/tech/170.txt           0.256800 -0.076643 -0.005060 -0.063172   \n",
       "data/bbc/tech/302.txt           0.178142  0.275478 -0.044081  0.069659   \n",
       "data/bbc/tech/256.txt           0.113356 -0.150963 -0.054547  0.078308   \n",
       "data/bbc/tech/211.txt           0.161718  0.072896 -0.033089 -0.112782   \n",
       "...                                  ...       ...       ...       ...   \n",
       "data/bbc/entertainment/015.txt -0.188401 -0.300901  0.023362 -0.128732   \n",
       "data/bbc/entertainment/345.txt -0.178258 -0.212788  0.019289 -0.097152   \n",
       "data/bbc/entertainment/024.txt -0.376309 -0.317978  0.054523 -0.213948   \n",
       "data/bbc/entertainment/299.txt  0.129563 -0.149498 -0.086580 -0.084570   \n",
       "data/bbc/entertainment/300.txt -0.188012 -0.146533 -0.042753 -0.151030   \n",
       "\n",
       "                                    4         5         6         7     \\\n",
       "data/bbc/tech/174.txt           0.144047  0.166769  0.017964  0.342860   \n",
       "data/bbc/tech/170.txt           0.138440  0.189914  0.182261  0.326142   \n",
       "data/bbc/tech/302.txt           0.219834  0.160469 -0.008855  0.358277   \n",
       "data/bbc/tech/256.txt           0.010922  0.414599 -0.085480  0.392278   \n",
       "data/bbc/tech/211.txt           0.426878  0.299916  0.084742  0.284771   \n",
       "...                                  ...       ...       ...       ...   \n",
       "data/bbc/entertainment/015.txt  0.306804  0.241635  0.068264  0.128954   \n",
       "data/bbc/entertainment/345.txt  0.224833  0.122836 -0.037274 -0.269267   \n",
       "data/bbc/entertainment/024.txt  0.229501  0.119958  0.037880 -0.051839   \n",
       "data/bbc/entertainment/299.txt  0.133638  0.179378  0.126872  0.171507   \n",
       "data/bbc/entertainment/300.txt  0.167452  0.264067 -0.069800  0.476115   \n",
       "\n",
       "                                    8         9     ...      1014      1015  \\\n",
       "data/bbc/tech/174.txt          -0.161670 -0.082526  ... -0.340656  0.244855   \n",
       "data/bbc/tech/170.txt          -0.117797  0.026969  ... -0.171903  0.165076   \n",
       "data/bbc/tech/302.txt           0.126847 -0.048421  ... -0.290554  0.097650   \n",
       "data/bbc/tech/256.txt           0.147020 -0.126910  ... -0.291242  0.131670   \n",
       "data/bbc/tech/211.txt          -0.164555  0.051139  ... -0.091179  0.205737   \n",
       "...                                  ...       ...  ...       ...       ...   \n",
       "data/bbc/entertainment/015.txt -0.128111 -0.200275  ... -0.253069  0.376614   \n",
       "data/bbc/entertainment/345.txt -0.330137 -0.047496  ... -0.123795  0.075508   \n",
       "data/bbc/entertainment/024.txt -0.106681 -0.058162  ... -0.211875  0.328167   \n",
       "data/bbc/entertainment/299.txt  0.190518 -0.126271  ... -0.183596  0.280337   \n",
       "data/bbc/entertainment/300.txt  0.135215 -0.078510  ... -0.133540  0.364964   \n",
       "\n",
       "                                    1016      1017      1018      1019  \\\n",
       "data/bbc/tech/174.txt           0.030709  0.253689  0.078406  0.044581   \n",
       "data/bbc/tech/170.txt          -0.113165  0.327153  0.209255  0.082908   \n",
       "data/bbc/tech/302.txt          -0.033120  0.317354 -0.011647  0.147325   \n",
       "data/bbc/tech/256.txt           0.126649  0.161829  0.085308  0.041790   \n",
       "data/bbc/tech/211.txt          -0.067422  0.088594  0.118239  0.079229   \n",
       "...                                  ...       ...       ...       ...   \n",
       "data/bbc/entertainment/015.txt -0.215489  0.362154 -0.009835 -0.058118   \n",
       "data/bbc/entertainment/345.txt -0.240921  0.167021  0.135244  0.192209   \n",
       "data/bbc/entertainment/024.txt -0.178842  0.107765 -0.138602 -0.028439   \n",
       "data/bbc/entertainment/299.txt -0.227419  0.318273  0.012311  0.118652   \n",
       "data/bbc/entertainment/300.txt -0.131337  0.285388  0.203801  0.129833   \n",
       "\n",
       "                                    1020      1021      1022      1023  \n",
       "data/bbc/tech/174.txt           0.372656 -0.006501  0.273694  0.057726  \n",
       "data/bbc/tech/170.txt           0.258461 -0.122383  0.307110 -0.029122  \n",
       "data/bbc/tech/302.txt           0.441159 -0.065567  0.119717 -0.027908  \n",
       "data/bbc/tech/256.txt           0.132907 -0.010835  0.489633 -0.170547  \n",
       "data/bbc/tech/211.txt           0.402150 -0.011402  0.355481 -0.067534  \n",
       "...                                  ...       ...       ...       ...  \n",
       "data/bbc/entertainment/015.txt  0.197030  0.056924  0.493857 -0.102432  \n",
       "data/bbc/entertainment/345.txt  0.115502 -0.237828  0.562652 -0.099549  \n",
       "data/bbc/entertainment/024.txt  0.088771 -0.069143  0.444389 -0.067932  \n",
       "data/bbc/entertainment/299.txt  0.127632 -0.050924  0.435821 -0.108651  \n",
       "data/bbc/entertainment/300.txt -0.031789 -0.225053  0.574074 -0.091809  \n",
       "\n",
       "[2225 rows x 1024 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(os.path.join('elmo_embeddings', 'elmo_embeddings.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f867d719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Run Date: Thursday, January 19, 2023\n",
      "# Run Time: 00:07:17\n"
     ]
    }
   ],
   "source": [
    "endTime = time.time()\n",
    "elapsedTime = time.strftime(\"%H:%M:%S\", time.gmtime(endTime - startTime))\n",
    "\n",
    "print(todaysDate.strftime('# Run Date: %A, %B %d, %Y'))\n",
    "print(f\"# Run Time: {elapsedTime}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
